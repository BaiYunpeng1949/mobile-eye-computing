{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537937a8",
   "metadata": {},
   "source": [
    "# The Pupil Raw Features Extracted in Batch Pipeline.\n",
    "## Introuduction\n",
    "The features to be extracted includes blinking rate and chunks of wavelet coefficients.\n",
    "\n",
    "## Reference\n",
    "1. The previous codes.https://github.com/BaiYunpeng1949/MobileEyeComputing/tree/master/ProcessDataNTestAlgorithm\n",
    "2. TODO: add papers here.\n",
    "3. My work: https://docs.google.com/document/d/1oLv3oJQLjst1_pYgd_UA3RRL1fRGSbZ6uvlMuxmZR2k/edit#heading=h.r01ccf7ox05g\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75874ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.special import logsumexp\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from fastdtw import fastdtw\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import datetime\n",
    "import csv\n",
    "import os, sys, warnings\n",
    "import pywt, math\n",
    "import argparse\n",
    "import scipy\n",
    "from numba import jit\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62025f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(ax, data1, data2, param_dict):\n",
    "    out = ax.plot(data1, data2, **param_dict)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c74efe",
   "metadata": {},
   "source": [
    "## File-wise/Task-wise Data Pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d6d80d",
   "metadata": {},
   "source": [
    "### Constants Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b7c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# Utilized in manipulating two eyes' data.\n",
    "TS = 'Timestamp'\n",
    "CF = 'Confidence'\n",
    "DM = 'Diameter'\n",
    "LDM = 'Left Diameter'\n",
    "RDM = 'Right Diameter'\n",
    "LCF = 'Left Confidence'\n",
    "RCF = 'Right Confidence'\n",
    "AVEDM = 'Average Diameter'\n",
    "AVECF = 'Average Confidence'\n",
    "DIFFDM = 'Difference Diameter'  # By my default: Left - Right - unit in pixels.\n",
    "SR = 'Averaged Sampling Rate'\n",
    "EVENT = 'Event'\n",
    "TARGET_TASK = 'sitting'\n",
    "\n",
    "# Interpolation.\n",
    "INTERPOLATE_TYPE = 'linear'\n",
    "LIMIT_DIRECTION = 'both'\n",
    "\n",
    "#######################################################################################\n",
    "# Used in the de-blinking part.\n",
    "# Deblink.\n",
    "BLINK_EXTENSION_TIMEWINDOW = 0.2 # 200ms\n",
    "MIN_CF = 0.25\n",
    "MIN_NUM_SAMPLE_BLINKS = 2\n",
    "\n",
    "# Smooth.\n",
    "WIN_TYPE = 'hann'\n",
    "HANN_WINDOW_SIZE = 5\n",
    "\n",
    "# Interpolate.\n",
    "CURVE_TYPE = 'linear'\n",
    "CURVE_ORDER = 3\n",
    "\n",
    "# Column configuration.\n",
    "ISBLINK = 'isBlink'\n",
    "ISBLINK_LEFT = 'isBlink-Left'\n",
    "ISBLINK_RIGHT = 'isBlink-Right'\n",
    "\n",
    "AVE_DM = 'Averaged Diameter'\n",
    "DIFF_DM = 'Difference Diameter'\n",
    "\n",
    "# Used in the artifact rejection part.\n",
    "HAMPLE_WIN_SIZE = 10\n",
    "\n",
    "#######################################################################################\n",
    "# Used in segmenting the time windows.\n",
    "SAMPLING_RATE = 120 # The unit is Hertz.\n",
    "\n",
    "# Used in the feature generating parts.\n",
    "wavelet_decomposition_level = 2\n",
    "wavelet = 'sym16'\n",
    "\n",
    "IPA_LEFT = 'IPA Left'\n",
    "IPA_RIGHT = 'IPA Right'\n",
    "\n",
    "LHIPA_LEFT = 'LHIPA Left'\n",
    "LHIPA_RIGHT = 'LHIPA Right'\n",
    "\n",
    "MEAN_LEFT = 'Mean Left'\n",
    "MEAN_RIGHT = 'Mean Right'\n",
    "\n",
    "STD_LEFT = 'STD Left'\n",
    "STD_RIGHT = 'STD Right'\n",
    "\n",
    "SKEW_LEFT = 'Skew Left'\n",
    "SKEW_RIGHT = 'Skew Right'\n",
    "\n",
    "MAX_LEFT = 'MAX Left'\n",
    "MAX_RIGHT = 'MAX Right'\n",
    "\n",
    "MED_LEFT = 'Med Left'\n",
    "MED_RIGHT = 'Med Right'\n",
    "\n",
    "VAR_LEFT = 'Var Left'\n",
    "VAR_RIGHT = 'Var Right'\n",
    "\n",
    "# Experimental conditions\n",
    "# Luminance.\n",
    "LUX = 'Luminance'\n",
    "# Task difficulty - labels.\n",
    "LABELS = 'Labels'\n",
    "PID = 'PID'\n",
    "\n",
    "#######################################################################################\n",
    "# Used in the run-in-batch part.\n",
    "# File read configurations.\n",
    "TWODMODE = '2D'\n",
    "LEFT = 'left'\n",
    "RIGHT = 'right'\n",
    "\n",
    "# Experimental conditions\n",
    "# Luminance.\n",
    "LOW = 'lowlux'\n",
    "MID = 'middlelux'\n",
    "HIGH = 'highlux'\n",
    "LUXS_SET = [LOW, MID, HIGH]\n",
    "# Task difficulty - labels.\n",
    "NOBACK = 'nothing'\n",
    "ONEBACK = 'ONEBACK'\n",
    "TWOBACK = 'TWOBACK'\n",
    "THREEBACK = 'THREEBACK'\n",
    "TASKDIFFS_SET = [NOBACK, ONEBACK, TWOBACK, THREEBACK]\n",
    "\n",
    "#######################################################################################\n",
    "# Configuration for Time-Series Cross Validation.\n",
    "TRAIN_TEST_SPLIT = LABELS + ' Train and Test'\n",
    "VALIDATION_SPLIT = LABELS + ' Validation'\n",
    "TRAIN = 'train'\n",
    "VALIDATION = 'val'\n",
    "TEST = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cc161",
   "metadata": {},
   "source": [
    "### [Suspended] Left and Right Eye Synchronization and Calculate the Difference\n",
    "Since I now implement ML/AI models to do a more inclusive analysis where more features could be analyzed, I will use the feature of the difference between left and right eye data.\n",
    "\n",
    "\n",
    "1. Referenced from the previous work located as: https://github.com/BaiYunpeng1949/MobileEyeComputing/blob/master/ProcessDataNTestAlgorithm/LeftRightEyesSyncData.ipynb\n",
    "\n",
    "2. The difference between two eyes were found correlated with cognitive workload estimation as well, see the related paper: Optimizing the usage of pupillary based indicators for cognitive workload. Reading link: https://docs.google.com/document/d/1jBezc9kqaziGlWk6sSgCvyjHTlpD5G6OoNZyh7K2HIo/edit#heading=h.qqfv1ot6zjd8\n",
    "\n",
    "Besides, there is a considerable difference in pupil size variation for right and left eyes of the participants. See the paper: Exploring pupil size variation as a cognitive load indicator in visualization studies, link: https://drive.google.com/file/d/1z8O1NGNYA87La-CVMSr7cQkJ-VVOOUSe/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312ae7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _pre_dtw_sync(left_eye_file_path, right_eye_file_path):   \n",
    "#     # Configure the parameters\n",
    "#     entity_dwt_align = 'Timestamp'\n",
    "#     entity_dwt_apply_dia = 'Diameter'\n",
    "#     entity_dwt_apply_conf = 'Confidence'\n",
    "#     left_eye = 'left'\n",
    "#     right_eye = 'right'\n",
    "    \n",
    "#     # Tag labels\n",
    "#     SAMPLING_RATE_LEFT = int((left_eye_file_path.split('_')[-1]).split('Hz')[0])\n",
    "#     data_left = pd.read_csv(left_eye_file_path)\n",
    "\n",
    "#     SAMPLING_RATE_RIGHT = int((right_eye_file_path.split('_')[-1]).split('Hz')[0])\n",
    "#     data_right = pd.read_csv(right_eye_file_path)\n",
    "    \n",
    "#     df_left = data_left[['Timestamp','Confidence','Diameter','Event']].copy()\n",
    "#     df_right = data_right[['Timestamp','Confidence','Diameter','Event']].copy()\n",
    "    \n",
    "#     # Determine the left and right eye data's size first: which one is bigger?\n",
    "#     # Identify the number of elements in the left/right eyes data. If one applies the up-sampling method, the larger eye needs to be put in the first argument.\n",
    "#     len_left = len(df_left)\n",
    "#     len_right = len(df_right)\n",
    "#     if len_left >= len_right:\n",
    "#         df_reference = df_left.copy()  # df_reference: the one that being put in the first argument, as a reference.\n",
    "#         df_alignment = df_right.copy() # df_alignment: the one that being put in the second argument, to be aligned to the reference.\n",
    "#         df_origin = df_left.copy()\n",
    "#         SR_SYNC = SAMPLING_RATE_LEFT\n",
    "#     elif len_left < len_right:\n",
    "#         df_reference = df_right.copy()\n",
    "#         df_alignment = df_left.copy()\n",
    "#         df_origin = df_right.copy()\n",
    "#         SR_SYNC = SAMPLING_RATE_RIGHT\n",
    "    \n",
    "#     # Calculate for warping.\n",
    "#     distance, path = fastdtw(df_reference[entity_dwt_align], df_alignment[entity_dwt_align], dist=euclidean)\n",
    "    \n",
    "#     return path, df_reference, df_alignment, entity_dwt_apply_dia, entity_dwt_apply_conf, df_origin, SR_SYNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab963e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function scnchronize and align/merge 2 eyes' numerical values, including diamter and confidence values.\n",
    "# # The method of getting the average/mean value of 2 eyes' data as computing target is referenced from the mention in LHIPA.\n",
    "# def _dtw_synchronize_merge_data(path_dwt, df_reference, df_alignment, entity_dwt_apply):\n",
    "#     # Synchronize\n",
    "#     data_sync = []\n",
    "#     for i in range(0, len(path_dwt)):\n",
    "#         data_sync.append([path_dwt[i][0],  # The index column is for dropping out duplicates.\n",
    "#                          df_reference[entity_dwt_apply].iloc[path_dwt[i][0]],\n",
    "#                          df_alignment[entity_dwt_apply].iloc[path_dwt[i][1]]])\n",
    "#     df_sync = pd.DataFrame(data=data_sync,\n",
    "#                            columns=['Index', \n",
    "#                                     'Reference '+entity_dwt_apply, \n",
    "#                                     'Alignment '+entity_dwt_apply]).dropna()\n",
    "#     df_sync = df_sync.drop_duplicates(subset=['Index']) # Drop the duplicates according to the index of the reference.\n",
    "#     df_sync = df_sync.reset_index(drop=True)\n",
    "#     # Merge/Align\n",
    "#     df_sync['Avg'+entity_dwt_apply] = df_sync.loc[:, ['Reference '+entity_dwt_apply, 'Alignment '+entity_dwt_apply]].mean(axis = 1)\n",
    "#     # Calculate the difference\n",
    "#     df_sync['Diff'+entity_dwt_apply] = df_sync['Reference '+entity_dwt_apply] - df_sync['Alignment '+entity_dwt_apply]\n",
    "    \n",
    "#     return df_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b321f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Synchronize the given 2 eyes' data.\n",
    "# def dtw_sync(left_eye_file_path, right_eye_file_path):\n",
    "#     # Prepare for the sychronization.\n",
    "#     path, df_reference, df_alignment, entity_dwt_apply_dia, entity_dwt_apply_conf, df_origin, SR_SYNC = _pre_dtw_sync(left_eye_file_path = left_eye_file_path, \n",
    "#                                                                                                                   right_eye_file_path = right_eye_file_path)\n",
    "\n",
    "    \n",
    "#     # Synchronize, merge, and label data.\n",
    "#     df_sync_dia = _dtw_synchronize_merge_data(path_dwt=path,\n",
    "#                                               df_reference=df_reference,\n",
    "#                                               df_alignment=df_alignment,\n",
    "#                                               entity_dwt_apply=entity_dwt_apply_dia)\n",
    "#     df_sync_conf = _dtw_synchronize_merge_data(path_dwt=path,\n",
    "#                                                df_reference=df_reference,\n",
    "#                                                df_alignment=df_alignment,\n",
    "#                                                entity_dwt_apply=entity_dwt_apply_conf)\n",
    "    \n",
    "#     # Integrate into one dataframe.\n",
    "#     df_sync = pd.DataFrame()\n",
    "#     df_sync['Timestamp'] = df_origin['Timestamp']\n",
    "#     df_sync['Confidence'] = df_sync_conf['AvgConfidence']\n",
    "#     df_sync['Diameter'] = df_sync_dia['AvgDiameter']\n",
    "#     df_sync['Event'] = df_origin['Event']\n",
    "#     df_sync['DiffDiameter'] = df_sync_dia['DiffDiameter']\n",
    "    \n",
    "#     # Output and save into a csv file.\n",
    "#     df_export = df_sync.copy()\n",
    "#     file_name = left_eye_file_path.split('/')[-2:]\n",
    "\n",
    "#     folder_path = '../Data/PreprocessedData/' + file_name[0] + '/'\n",
    "#     if os.path.exists(folder_path) is False:\n",
    "#         os.makedirs(folder_path)\n",
    "\n",
    "#     write_file_name = 'synchronized_' + str(SR_SYNC) + 'Hz.csv'\n",
    "#     write_file_path = folder_path + write_file_name\n",
    "#     df_export.to_csv(write_file_path)\n",
    "    \n",
    "#     return df_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896c170",
   "metadata": {},
   "source": [
    "### Merge Two Eyes' Data using Timestamps, Interpolate, and Calculate\n",
    "Reference: a blog introducing 2 sensors' data fusion - https://stackoverflow.com/questions/14079766/synchronize-dataset-multiple-users-multiple-timestamps\n",
    "\n",
    "#### Implementation\n",
    "To be noted that, when using the interpolation, we need to avoid overshooting, which is easily caused by spline curves. Otherwise we will have plenty negative pupil diameters and large outliers. I would use the 'linear' method to interpolate as suggested by Marshall. Apart from that, one might also want to try [monotonic](https://stackoverflow.com/questions/40072420/interpolate-without-having-negative-values-in-python) interpolators to avoid overshootings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed6af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_timestamps_sync(left_eye_file_path, right_eye_file_path):\n",
    "    # Read data from csv into dataframes.\n",
    "    df_left = pd.read_csv(left_eye_file_path)\n",
    "    df_right = pd.read_csv(right_eye_file_path)\n",
    "    df_left = df_left[[TS,CF,DM,EVENT]].copy()\n",
    "    df_right = df_right[[TS,CF,DM,EVENT]].copy()\n",
    "\n",
    "    # Collect data from the targetted events: sitting.\n",
    "    df_left = df_left.loc[df_left[EVENT] == TARGET_TASK]\n",
    "    df_right = df_right.loc[df_right[EVENT] == TARGET_TASK]\n",
    "#     print(len(df_left[df_left[EVENT]=='default']))\n",
    "#     print(len(df_right[df_right[EVENT]=='default']))\n",
    "    \n",
    "    # Get the diameter data indexed by timestamps.\n",
    "    left_diameters = df_left[DM].to_numpy() \n",
    "    series_left = pd.Series(left_diameters, index=df_left[TS])\n",
    "    right_diameters = df_right[DM].to_numpy() \n",
    "    series_right = pd.Series(right_diameters, index=df_right[TS])\n",
    "    \n",
    "    # Synchronize 2 eyes' data by listing all timestamps, this process is actually a up-sampling.\n",
    "    df_sync = pd.DataFrame([series_left, series_right]).T.sort_index()\n",
    "    df_sync = df_sync.rename(columns={0: LDM, 1: RDM})\n",
    "\n",
    "    #Interpolate all the NAN values using 'Spline 3' method with a bi-directional strategy to fill both the first and the last NAN values.\n",
    "    df_sync = df_sync.interpolate(method=INTERPOLATE_TYPE, limit_direction=LIMIT_DIRECTION, axis=0)\n",
    "#     df_sync = df_sync.ffill()\n",
    "    \n",
    "    # Align the confidence values according to the timestamps. \n",
    "    # Reference: Adding values in new column based on indexes with pandas in python. https://stackoverflow.com/questions/45636105/adding-values-in-new-column-based-on-indexes-with-pandas-in-python\n",
    "    df_sync[LCF] = df_sync.index.to_series().map(df_left.set_index(TS)[CF])\n",
    "    df_sync[RCF] = df_sync.index.to_series().map(df_right.set_index(TS)[CF])\n",
    "    \n",
    "    # Interpolate NAN values using the normal linear method.\n",
    "    df_sync[LCF] = df_sync[LCF].interpolate(method=INTERPOLATE_TYPE, limit_direction=LIMIT_DIRECTION, axis=0)\n",
    "    df_sync[RCF] = df_sync[RCF].interpolate(method=INTERPOLATE_TYPE, limit_direction=LIMIT_DIRECTION, axis=0)\n",
    "    \n",
    "#     # Get the difference and average of two eyes' diameter data and confidence values.\n",
    "#     df_sync[AVEDM] = (df_sync[LDM] + df_sync[RDM]) / 2\n",
    "#     df_sync[DIFFDM] = df_sync[LDM] - df_sync[RDM]\n",
    "#     df_sync[AVECF] = (df_sync[LCF] + df_sync[RCF]) / 2\n",
    "    \n",
    "    # Get a new column storing the current trial's averaged sampling rate (the up-sampled version by interpolation).\n",
    "    ave_samp_rate = len(df_sync) / (df_sync.index[-1] - df_sync.index[0])\n",
    "    df_sync.loc[:, SR] = ave_samp_rate\n",
    "    df_sync = df_sync.copy()\n",
    "    \n",
    "    return df_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df6e65",
   "metadata": {},
   "source": [
    "### Deblinks and Blinking Rate Extraction\n",
    "\n",
    "Reference: Check how David Linderbaure's group deal with the blinks. Use confidence to identify blinks. They removed the data within 200ms. However, they did not interpolate the eliminated ones. Here I clean data before and after 200ms of blinks. The input is the numpy data list of the \"confidence\" conlumn. Then return a list that marks which indecies are blinks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734bcb1",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f12eec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deblinks(df_input, confidence_column_label, diameter_column_label, isblink_column_label):\n",
    "    # Feed in the dataframe to be processed and specific columns of the same eye/the averaged eye.\n",
    "    # To be noted that the input was indexed by the timestamps. \n",
    "    # One Has to use the form of df_input[Col][df_input.index[i]] to reach the ith element.\n",
    "    df = df_input.copy()\n",
    "    # Initiate all 0 values to the IsBlink column.\n",
    "    df.loc[:,isblink_column_label] = 0\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Parameter initilization\n",
    "    blinks = []\n",
    "    num_samples = len(df)\n",
    "    i = 0 # The index starter.\n",
    "    \n",
    "    # Identify the blinks according to the low confidence values.\n",
    "    while i < num_samples:\n",
    "        if df[confidence_column_label][df.index[i]] < MIN_CF and i < num_samples -1:\n",
    "            offset = 1\n",
    "            next_data = df[confidence_column_label][df.index[i+offset]]\n",
    "            while next_data < MIN_CF:\n",
    "                offset = offset + 1\n",
    "                if i + offset >= (num_samples - 1): # Check wheter exceeding the indecies boundary.\n",
    "                    break\n",
    "                next_data = df[confidence_column_label][df.index[i+offset]]\n",
    "            \n",
    "            # Judge whether the current index exceeds the 200ms time window.\n",
    "            if offset >= MIN_NUM_SAMPLE_BLINKS:\n",
    "                blinks.append((i, offset))\n",
    "            \n",
    "            i = i + offset\n",
    "        else:\n",
    "            i = i + 1\n",
    "    \n",
    "    # Mark data before and after BLINK_EXTENSION_TIMEWINDOW of samples.\n",
    "    for j in range(len(blinks)):\n",
    "        blink_index = blinks[j][0]\n",
    "        blink_length = blinks[j][1]\n",
    "        \n",
    "        # Mark blinks within the searched area as np.nan values.\n",
    "        for j in range(0, blink_length):\n",
    "            df[diameter_column_label][df.index[blink_index + j]] = np.nan # Flag an NAN for the blinkings' diameters.\n",
    "            df[isblink_column_label][df.index[blink_index + j]] = 1 # Flag a numerical value 1 for the blinkings.\n",
    "        \n",
    "        # Search for the time window with a length of 200ms. Then also mark blinks with np.nan values for the convenience of interplating.\n",
    "        # Decremnenting.\n",
    "        blink_start_timestamp = df.index[blink_index]\n",
    "        k_dec = 0\n",
    "        decrement_index = blink_index - k_dec\n",
    "        # Controlled by the boundary conditions.\n",
    "        while decrement_index >= 0:\n",
    "            dec_timestamp = df.index[decrement_index]\n",
    "            if blink_start_timestamp - dec_timestamp >= BLINK_EXTENSION_TIMEWINDOW:\n",
    "                break\n",
    "            else:\n",
    "                df[diameter_column_label][df.index[decrement_index]] = np.nan # Set an NAN flag for data processing on the blinking data.\n",
    "                df[isblink_column_label][df.index[decrement_index]] = 1 # Set an numerical flag on the blinking data.\n",
    "                k_dec = k_dec + 1\n",
    "                decrement_index = blink_index - k_dec\n",
    "        \n",
    "        # Incrementing - check the boundary limits first.\n",
    "        blink_stop_timestamp = df.index[blink_index + blink_length]\n",
    "        k_inc = 0\n",
    "        increment_index = blink_index + blink_length + k_inc\n",
    "        while increment_index < num_samples:\n",
    "            inc_timestamp = df.index[increment_index]\n",
    "            if inc_timestamp - blink_stop_timestamp >= BLINK_EXTENSION_TIMEWINDOW:\n",
    "                break\n",
    "            else:\n",
    "                df[diameter_column_label][df.index[increment_index]] = np.nan # Set an NAN flag for data processing on the blinking data.\n",
    "                df[isblink_column_label][df.index[increment_index]] = 1 # Set an numerical flag on the blinking data.\n",
    "                k_inc = k_inc + 1\n",
    "                increment_index = blink_index + blink_length + k_inc\n",
    "    \n",
    "    \n",
    "    # Smooth the data - [Suspended] - Since the objective of freqeuncy-based analysis was to detect singularities, smooth should not be included here.\n",
    "    # Besides, in Marshall's patent, https://patentimages.storage.googleapis.com/91/2f/5f/236d6711dcf6b6/US6090051.pdf, smooth was not applied.\n",
    "#     df[diameter_column_label] = df[diameter_column_label].rolling(window=HANN_WINDOW_SIZE, center=True, win_type=WIN_TYPE).mean()\n",
    "    \n",
    "    # Interpolate the data - included in Marshall's patent - however the interpolation would introduce a lot of large negative values.\n",
    "    df[diameter_column_label] = df[diameter_column_label].interpolate(method=CURVE_TYPE,order=CURVE_ORDER, limit_direction='both', axis=0)\n",
    "    \n",
    "    df_output = df.copy()\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87d550",
   "metadata": {},
   "source": [
    "### Artefact Rejection\n",
    "\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "900859ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This part is directly cited from Sam's work.\n",
    "\n",
    "# Filtering outliers \n",
    "# Lan et al. 2020 - median filter with sliding window of 10s\n",
    "# Testing with numba optimised for-loop implementation of a Hampel Filter\n",
    "# Note to self: I think this filter is also commonly used for pupil diameter filtering\n",
    "@jit(nopython=True)\n",
    "def hampel_filter_forloop_numba(input_series, window_size, n_sigmas=3):\n",
    "    \n",
    "    n = len(input_series)\n",
    "    new_series = input_series.copy()\n",
    "    k = 1.4826 # scale factor for Gaussian distribution\n",
    "    indices = []\n",
    "    \n",
    "    for i in range((window_size),(n - window_size)):\n",
    "        x0 = np.nanmedian(input_series[(i - window_size):(i + window_size)])\n",
    "        S0 = k * np.nanmedian(np.abs(input_series[(i - window_size):(i + window_size)] - x0))\n",
    "        if (np.abs(input_series[i] - x0) > n_sigmas * S0):\n",
    "            new_series[i] = x0\n",
    "            indices.append(i)\n",
    "    \n",
    "    return new_series, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ad3e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rej_artifact(df_input, diameter_column_label):\n",
    "    df = df_input.copy()\n",
    "    x_, outlier_x_ = hampel_filter_forloop_numba(df[diameter_column_label].to_numpy(), HAMPLE_WIN_SIZE)\n",
    "    df[diameter_column_label] = x_.tolist()\n",
    "    df_output = df.copy()\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a406f9",
   "metadata": {},
   "source": [
    "## Time-series Data Visualization\n",
    "In this part, I visualize pupil data for different cognitive activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88c5b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pupil_diameters(df_input, title_label):\n",
    "    fig, ax = plt.subplots()\n",
    "    df_input[LDM].plot(ax=ax)  # Plot the left diameter data.\n",
    "    df_input[RDM].plot(ax=ax, title='Clean and Preprocessed Pupil Diameters (in pixels)\\n' + title_label)  # Plot the right diameter data.\n",
    "    ax.legend(['Left Eye', 'Right Eye'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155736b7",
   "metadata": {},
   "source": [
    "## Segmenting Time Windows\n",
    "\n",
    "In this part, I will use overlapped sliding windows to extract wavelet-related features.\n",
    "\n",
    "The reference publications include:\n",
    "1. [WiStress: Contactless Stress Monitoring Using Wireless Signals, 2021](https://dl.acm.org/doi/pdf/10.1145/3478121).\n",
    "2. [Feature extraction for robust physical activity recognition, 2017](https://hcis-journal.springeropen.com/articles/10.1186/s13673-017-0097-2).\n",
    "3. [Feature Engineering on Time-Series Data for Human Activity Recognition](https://towardsdatascience.com/feature-engineering-on-time-series-data-transforming-signal-data-of-a-smartphone-accelerometer-for-72cbe34b8a60).\n",
    "4. [Indexing Cognitive Workload Based on Pupillary Response under Luminance and Emotional Changes, 2013](https://dl-acm-org.libproxy1.nus.edu.sg/doi/pdf/10.1145/2449396.2449428).\n",
    "\n",
    "### Implementation\n",
    "The time window segmentation will be conducted file-wisely, i.e., each trial's data produces their own windows. Hence that to be aligned to the data pre-processing's file-wise fashion.\n",
    "\n",
    "By slicing data with overlapped sliding windows, the new instances will be created. The former instances are sample points, now is conjoint sliding window waiting to be transformed into multiple features described by wavelet decomposition.\n",
    "\n",
    "From here, the sample points are regarded as averaged ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c8bf1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I constructed by own overlapped sliding window here.\n",
    "def segment(args, df_input):\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    # Parameter initialization.\n",
    "    num_samples = len(df)\n",
    "    i=0 # Store the sliding window's starting point.\n",
    "    offset_next = 0 # Store the next sliding window's starting index.\n",
    "    windows = []\n",
    "    \n",
    "    window_length = args.time_window_length\n",
    "    overlap_length = args.overlap_length\n",
    "    window_num = int(SAMPLING_RATE * window_length)\n",
    "    overlap_num = int(SAMPLING_RATE * overlap_length)\n",
    "    step_num = int(window_num - overlap_num)\n",
    "    \n",
    "    # Determine the number of sample points according to the standard sampling rate, i.e., 120 Hz.\n",
    "    indices = np.arange(num_samples)\n",
    "    shape = (indices.size - window_num + 1, window_num)\n",
    "    stride = indices.strides * 2\n",
    "    view = np.lib.stride_tricks.as_strided(indices, strides = stride, shape = shape)[0::step_num]\n",
    "    output_2D_list = view.copy()\n",
    "    for i in range(len(output_2D_list)):\n",
    "        windows.append((output_2D_list[i][0], output_2D_list[i][-1]))\n",
    "    \n",
    "#     # Determine the number of sample points according to given sampling rate according to the timestamps.\n",
    "#     # I suspend this part, then use the standard data sampling part is for union data points and union feature expansion while using wavelet decomposition.\n",
    "#     windows = []\n",
    "#     while i < (num_samples - 1):\n",
    "#         starting_timestamp = df.index[i]\n",
    "#         offset = 1\n",
    "#         stopping_timestamp = df.index[i+offset]\n",
    "#         while stopping_timestamp - starting_timestamp < window_length:\n",
    "#             # Check whether reaches the overlapping edge.\n",
    "#             if stopping_timestamp - starting_timestamp <= (window_length - overlap_length):\n",
    "#                 offset_next = offset\n",
    "#                 # Then it should stop\n",
    "            \n",
    "#             offset = offset + 1\n",
    "#             if i + offset >= (num_samples - 1): # Check wheter exceeding the indecies boundary.\n",
    "#                 break\n",
    "#             stopping_timestamp = df.index[i+offset]\n",
    "            \n",
    "#         # Store the starting index and offset index.\n",
    "#         windows.append((i, i+offset))\n",
    "        \n",
    "#         # Update the starting index.\n",
    "#         i = i + offset_next\n",
    "        \n",
    "#     # Iteratively check whether the last window is too short, if it is shorter thant the overlapped area, then discard it.\n",
    "#     while True:\n",
    "#         last_starting_index = df.index[windows[-1][0]]\n",
    "#         last_ending_index = df.index[windows[-1][1]-1]\n",
    "#         last_time_window_length = last_ending_index - last_starting_index\n",
    "#         if last_time_window_length <= overlap_length:\n",
    "#             del windows[-1]\n",
    "#         else\n",
    "#             break\n",
    "    \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abbd51",
   "metadata": {},
   "source": [
    "## Wavelet Coefficient Extraction\n",
    "\n",
    "This part generates frequency features using wavelet analysis.\n",
    "\n",
    "And generate new instances, which is composed of several sample points from the time window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18889169",
   "metadata": {},
   "source": [
    "### Wavelet-based Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d895ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum modulus.\n",
    "def modmax(d):\n",
    "    # Compute signal modulus.\n",
    "    m = [0.0] * len(d)\n",
    "    for i in range(len(d)):\n",
    "        m[i] = math.fabs(d[i])\n",
    "\n",
    "    # If value is larger than both neighbours , and strictly larger than either , then it is a local maximum.\n",
    "    t = [0.0] * len(d)\n",
    "\n",
    "    for i in range(len(d)):\n",
    "        ll = m[i - 1] if i >= 1 else m[i]\n",
    "        oo = m[i]\n",
    "        rr = m[i + 1] if i < len(d) - 2 else m[i]\n",
    "\n",
    "        if (ll <= oo and oo >= rr) and (ll < oo or oo > rr):\n",
    "            # compute magnitude\n",
    "            t[i] = math.sqrt(d[i] ** 2)\n",
    "        else:\n",
    "            t[i] = 0.0\n",
    "    return t\n",
    "\n",
    "\n",
    "# Calculate IPA\n",
    "def calc_ipa(d, wavelet, duration):\n",
    "    # Initialize parameter.\n",
    "    wavelet = wavelet\n",
    "    \n",
    "    # obtain 2-level DWT of pupil diameter signal d\n",
    "    try:\n",
    "        (cA2,cD2,cD1) = pywt.wavedec(d, 'db8','per',level=2)\n",
    "    except ValueError:\n",
    "        return\n",
    "\n",
    "    # get signal duration (in seconds)\n",
    "    tt = duration\n",
    "\n",
    "    # using data from Pedrotti et al\n",
    "    # tt = 1.0\n",
    "\n",
    "    # normalize by 1=2j , j = 2 for 2-level DWT\n",
    "    cA2[:] = [x / math.sqrt(4.0) for x in cA2]\n",
    "    cD1[:] = [x / math.sqrt(2.0) for x in cD1]\n",
    "    cD2[:] = [x / math.sqrt(4.0) for x in cD2]\n",
    "\n",
    "    # detect modulus maxima , see Listing 2\n",
    "    cD2m = modmax(cD2)\n",
    "\n",
    "    # threshold using universal threshold lambda_univ = s*sqrt(p(2 log n))\n",
    "    lambda_univ = np.std(cD2m) * math.sqrt(2.0 * np.log2(len(cD2m)))\n",
    "    # where s is the standard deviation of the noise\n",
    "    cD2t = pywt.threshold(cD2m, lambda_univ, mode=\"hard\")\n",
    "\n",
    "    # compute IPA\n",
    "    ctr = 0\n",
    "    for i in range(len(cD2t)):\n",
    "        # print(cD2t[i])\n",
    "        if math.fabs(cD2t[i]) > 0:\n",
    "            ctr += 1\n",
    "\n",
    "    # print(ctr)\n",
    "    IPA = float(ctr) / tt\n",
    "    return IPA \n",
    "\n",
    "\n",
    "# Calculate L/H IPA.\n",
    "def calc_lhipa(d, wavelet, duration):\n",
    "    \"\"\"\n",
    "    Here d is a list.\n",
    "    \"\"\"\n",
    "    # Initialize parameter.\n",
    "    wavelet = wavelet\n",
    "    \n",
    "    # Find max decomposition level.\n",
    "    w = pywt.Wavelet(wavelet)\n",
    "    maxlevel = pywt.dwt_max_level(len(d), filter_len=w.dec_len)\n",
    "\n",
    "    # Set high and low frequency band indeces.\n",
    "    hif, lof = 1, int(maxlevel / 2)\n",
    "\n",
    "    # Get detail coefficients of pupil diameter signal d.\n",
    "    cD_H = pywt.downcoef('d', d, wavelet, 'per', level=hif)\n",
    "    cD_L = pywt.downcoef('d', d, wavelet, 'per', level=lof)\n",
    "\n",
    "    # Normalize by 1/ 2j.\n",
    "    cD_H[:] = [x / math.sqrt(2 ** hif) for x in cD_H]\n",
    "    cD_L[:] = [x / math.sqrt(2 ** lof) for x in cD_L]\n",
    "\n",
    "    # Obtain the LH:HF ratio.\n",
    "    cD_LH = cD_L\n",
    "    for i in range(len(cD_L)):\n",
    "        # I tried to solve the RuntimeWarning nan issue by referring to a stackoverflow blog here https://stackoverflow.com/questions/27784528/numpy-division-with-runtimewarning-invalid-value-encountered-in-double-scalars\n",
    "        if cD_L[i]== 0:\n",
    "            cD_LH[i] = 0 # Sometimes, cD_L[i] would be 0 and cD_H[int((2 ** lof) / (2 ** hif)) * i] would be nan, and RuntimeWarning would be raised.\n",
    "        else:\n",
    "            cD_LH[i] = cD_L[i] / cD_H[int((2 ** lof) / (2 ** hif)) * i]   # Used a '//' instead of '/' to make sure the index is an integer. This line is the original line.\n",
    "\n",
    "    # Detect modulus maxima , see Duchowski et al. [15].\n",
    "    cD_LHm = modmax(cD_LH)\n",
    "\n",
    "    # Threshold using universal threshold λuniv = σˆ (2logn), where σˆ is the standard deviation of the noise.\n",
    "    lambda_univ = np.std(cD_LHm) * math.sqrt(2.0*np.log2(len(cD_LHm)))\n",
    "    cD_LHt = pywt.threshold(cD_LHm, lambda_univ, mode=\"less\")\n",
    "\n",
    "    # Get signal duration (in seconds).\n",
    "    # tt = d[-1].timestamp() - d[0].timestamp()\n",
    "    tt = duration  # The unit was the second.\n",
    "\n",
    "    # Compute LHIPA.\n",
    "    ctr = 0\n",
    "    for i in range(len(cD_LHt)):\n",
    "        if math.fabs(cD_LHt[i]) > 0:\n",
    "            ctr += 1\n",
    "    LHIPA = float(ctr) / tt\n",
    "    return LHIPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446fce8f",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a1558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_analysis(args, df_input, windows_indices, j=wavelet_decomposition_level):\n",
    "    df = df_input.copy()\n",
    "    windows = windows_indices\n",
    "\n",
    "    # Parameter initialization.\n",
    "    freq_features_two_eyes = []\n",
    "    freq_features_left = []\n",
    "    freq_features_right = []\n",
    "    column_name_left = 'Left-'\n",
    "    column_name_right = 'Right-'\n",
    "    \n",
    "    ipas_left = []\n",
    "    ipas_right = []\n",
    "    lhipas_left = []\n",
    "    lhipas_right = []\n",
    "    \n",
    "    means_left = []\n",
    "    means_right = []\n",
    "    stds_left = []\n",
    "    stds_right = []\n",
    "    medians_left = []\n",
    "    medians_right = []\n",
    "    maxs_left = []\n",
    "    maxs_right = []\n",
    "    skews_left = []\n",
    "    skews_right = []\n",
    "    variances_left = []\n",
    "    variances_right = []\n",
    "    \n",
    "    blinking_rates_left = []\n",
    "    blinking_rates_right = []\n",
    "    aves= []\n",
    "    diffs = []\n",
    "    luxes = []\n",
    "    labels = []\n",
    "    pids = []\n",
    "    \n",
    "    for i in range(len(windows)):\n",
    "        starting_index = windows[i][0]\n",
    "        ending_index = windows[i][1] + 1\n",
    "        \n",
    "        # Get the left and right eyes' pupil diameters.\n",
    "        data_left = df[LDM].to_list()[starting_index:ending_index]\n",
    "        data_right = df[RDM].to_list()[starting_index:ending_index]\n",
    "        \n",
    "        # Get the frequency features.\n",
    "        (cA2_left, cD2, cD1) = pywt.wavedec(data_left, wavelet, 'per', level=j)\n",
    "        (cA2_right, cD2, cD1) = pywt.wavedec(data_right, wavelet, 'per', level=j)\n",
    "        \n",
    "        cA2_two_eyes = np.concatenate((cA2_left, cA2_right), axis=0)\n",
    "        \n",
    "        freq_features_left.append(cA2_left)\n",
    "        freq_features_right.append(cA2_right)\n",
    "        freq_features_two_eyes.append(cA2_two_eyes)\n",
    "        \n",
    "        # Get the IPA features for both eyes.\n",
    "        ipa_left = calc_ipa(d=data_left, wavelet=wavelet, duration=args.time_window_length)\n",
    "        ipa_right = calc_ipa(d=data_right, wavelet=wavelet, duration=args.time_window_length)\n",
    "        ipas_left.append(ipa_left)\n",
    "        ipas_right.append(ipa_right)\n",
    "        \n",
    "        # Get the LHIPA features for both eyes.\n",
    "        lhipa_left = calc_lhipa(d=data_left, wavelet=wavelet, duration=args.time_window_length)\n",
    "        lhipa_right = calc_lhipa(d=data_right, wavelet=wavelet, duration=args.time_window_length)\n",
    "        lhipas_left.append(lhipa_left)\n",
    "        lhipas_right.append(lhipa_right)\n",
    "        \n",
    "        # Get the averaged diameter feature and difference diameter features.\n",
    "        left_ave = sum(data_left) / len(data_left)\n",
    "        right_ave = sum(data_right) / len(data_right)\n",
    "        ave = (left_ave + right_ave) / 2\n",
    "        diff = left_ave - right_ave\n",
    "        aves.append(ave)\n",
    "        diffs.append(diff)\n",
    "        \n",
    "        # Get the pupil diameters statistical features. \n",
    "        data_left_np = np.array(data_left)\n",
    "        data_right_np = np.array(data_right)\n",
    "        # Reference: Eye Tracking-Based Stress Classification of Athletes in Virtual Reality, 2022.\n",
    "        # Features: mean, std, skewness, max, median, variance.\n",
    "        \n",
    "        mean_left = np.mean(data_left_np)\n",
    "        mean_right = np.mean(data_right_np)\n",
    "        means_left.append(mean_left)\n",
    "        means_right.append(mean_right)\n",
    "        \n",
    "        std_left = np.std(data_left_np)\n",
    "        std_right = np.std(data_right_np)\n",
    "        stds_left.append(std_left)\n",
    "        stds_right.append(std_right)\n",
    "        \n",
    "        med_left = np.median(data_left_np)\n",
    "        med_right = np.median(data_right_np)\n",
    "        medians_left.append(med_left)\n",
    "        medians_right.append(med_right)\n",
    "        \n",
    "        max_left = np.max(data_left_np)\n",
    "        max_right = np.max(data_right_np)\n",
    "        maxs_left.append(max_left)\n",
    "        maxs_right.append(max_right)\n",
    "        \n",
    "        skew_left = scipy.stats.skew(data_left_np, axis=0, bias=True)\n",
    "        skew_right = scipy.stats.skew(data_right_np, axis=0, bias=True)\n",
    "        skews_left.append(skew_left)\n",
    "        skews_right.append(skew_right)\n",
    "        \n",
    "        var_left = np.var(data_left_np, axis=0)\n",
    "        var_right = np.var(data_right_np, axis=0)\n",
    "        variances_left.append(var_left)\n",
    "        variances_right.append(var_right)\n",
    "        \n",
    "        # Get the blinking rate feature from both eyes. df[df['col'] == value\n",
    "        blinks_left = np.array(df[ISBLINK_LEFT].to_list()[starting_index:ending_index])\n",
    "        blinking_rate_left = (np.sum(blinks_left))/len(blinks_left)\n",
    "        blinking_rates_left.append(blinking_rate_left)\n",
    "        \n",
    "        blinks_right = np.array(df[ISBLINK_RIGHT].to_list()[starting_index:ending_index])\n",
    "        blinking_rate_right = (np.sum(blinks_right))/len(blinks_right)\n",
    "        blinking_rates_right.append(blinking_rate_right)\n",
    "        \n",
    "        # Get the luminance feature. df['City'].iat[0]\n",
    "        luxes.append(df[LUX].iat[0])\n",
    "        \n",
    "        # Get the PID.\n",
    "        pids.append(df[PID].iat[0])\n",
    "        \n",
    "        # Get the task label.\n",
    "        labels.append(df[LABELS].iat[0])\n",
    "        \n",
    "    # Add high dimension features into the dataframe. Set the columns\n",
    "    # From now, the instances are vertically conpacted by the sliding windows, but horizontally expaned.\n",
    "    # Frequency features.\n",
    "    horizontal_length_left = np.array(freq_features_left).shape[1]\n",
    "    horizontal_length_right = np.array(freq_features_right).shape[1]\n",
    "    left_features = []\n",
    "    for i in range(horizontal_length_left):\n",
    "        feature_name = column_name_left + str(i)\n",
    "        left_features.append(feature_name)\n",
    "        \n",
    "    right_features = []\n",
    "    for i in range(horizontal_length_right):\n",
    "        feature_name = column_name_right + str(i)\n",
    "        right_features.append(feature_name)\n",
    "    \n",
    "    feature_names = left_features + right_features\n",
    "    df_freq = pd.DataFrame(freq_features_two_eyes, columns=feature_names)\n",
    "    \n",
    "    # IPA and LHIPA features for both eyes.\n",
    "    df_freq[IPA_LEFT] = ipas_left\n",
    "    df_freq[IPA_RIGHT] = ipas_right\n",
    "    df_freq[LHIPA_LEFT] = lhipas_left\n",
    "    df_freq[LHIPA_RIGHT] = lhipas_right\n",
    "    \n",
    "    # Diameter average and difference features.\n",
    "    df_freq[AVE_DM] = aves\n",
    "    df_freq[DIFF_DM] = diffs\n",
    "    \n",
    "    # Left and right eye diameters' statistical features.\n",
    "    df_freq[MEAN_LEFT] = means_left\n",
    "    df_freq[MEAN_RIGHT] = mean_right\n",
    "    \n",
    "    df_freq[MAX_LEFT] = maxs_left\n",
    "    df_freq[MAX_RIGHT] = maxs_right\n",
    "    \n",
    "    df_freq[STD_LEFT] = stds_left\n",
    "    df_freq[STD_RIGHT] = stds_right\n",
    "    \n",
    "    df_freq[SKEW_LEFT] = skews_left\n",
    "    df_freq[SKEW_RIGHT] = skews_right\n",
    "    \n",
    "    df_freq[MED_LEFT] = medians_left\n",
    "    df_freq[MED_RIGHT] = medians_right\n",
    "    \n",
    "    df_freq[VAR_LEFT] = variances_left\n",
    "    df_freq[VAR_RIGHT] = variances_right\n",
    "               \n",
    "    # Blinking rate features.\n",
    "    df_freq[ISBLINK_LEFT] = blinking_rates_left\n",
    "    df_freq[ISBLINK_RIGHT] = blinking_rates_right\n",
    "    \n",
    "    # Luminance features.\n",
    "    df_freq[LUX] = luxes\n",
    "    \n",
    "    # PID.\n",
    "    df_freq[PID] = pids\n",
    "    \n",
    "    # Label.\n",
    "    df_freq[LABELS] = labels\n",
    "        \n",
    "    df_output = df_freq.copy()\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc4d1fb",
   "metadata": {},
   "source": [
    "## Split the Dataset: Train, Validation, and Test\n",
    "\n",
    "In this part I split the dataset into training part, validation part, and testing parts. To get ready for deep learning related methods where validations are necessary for updating parameters.\n",
    "\n",
    "Since our model could be classified into 'Activity Recognition' and 'Time-series Forecasting Models', I followed their ways to split dataset to aviod dependancy problems / look-ahead bias. We should alwasy respect the temporal order in the dataset.Here I chose to use the 'walk forward validation' which is one of the most popular methods. The other possible solutions are 'Rep-Holdout' and 'hv holdout'.\n",
    "\n",
    "### Reference\n",
    "1. [Multiple time series forecasting: How to split the data for training of a neural network [duplicate]](https://stats.stackexchange.com/questions/564867/multiple-time-series-forecasting-how-to-split-the-data-for-training-of-a-neural)\n",
    "2. [Using k-fold cross-validation for time-series model selection.](https://stats.stackexchange.com/questions/14099/using-k-fold-cross-validation-for-time-series-model-selection)\n",
    "3. [Splitting Time Series Data into Train/Test/Validation Sets.](https://stats.stackexchange.com/questions/346907/splitting-time-series-data-into-train-test-validation-sets)\n",
    "4. [Stackoverflow: how to implement walk forward testing in sklearn?](https://stackoverflow.com/questions/31947183/how-to-implement-walk-forward-testing-in-sklearn)\n",
    "5. [sklearn.model_selection.TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a81906f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function label data with 'train', 'val', or 'test' to indicate their belonging groups.\n",
    "def time_series_cross_validation(args, df_input):\n",
    "    df = df_input.copy()\n",
    "    # Split the test dataset first: get the most recent data.\n",
    "    num_splits_test = (int(1 / args.test_pct) - 1) # To be noted that the split is 1 less than the partitions.\n",
    "    tscv_test = TimeSeriesSplit(n_splits=num_splits_test)\n",
    "    original_indecies = np.array(df.index)\n",
    "    # Since the labels are attached with the features in the 'Labels' column, I don't need to split the labels specifically.\n",
    "    counter = 0\n",
    "    for train_indecies, test_indecies in tscv_test.split(X=df):\n",
    "        counter = counter + 1\n",
    "        # Only get the last indecies set for testing.\n",
    "        if counter == num_splits_test:\n",
    "            df.loc[train_indecies,TRAIN_TEST_SPLIT] = TRAIN\n",
    "            df.loc[test_indecies,TRAIN_TEST_SPLIT] = TEST\n",
    "            \n",
    "            train_set_indecies = train_indecies\n",
    "            test_set_indecies = test_indecies\n",
    "    \n",
    "    # Split the validation dataset in the remaining part.\n",
    "    num_splits_val = int((1 - args.test_pct) / args.val_pct) - 1\n",
    "    tscv_val = TimeSeriesSplit(n_splits=num_splits_val)\n",
    "    \n",
    "    index_validation = 0\n",
    "    for train_indecies, val_indecies in tscv_val.split(X=train_set_indecies):\n",
    "        # Spliting and labeling.\n",
    "        current_val_labels = VALIDATION_SPLIT + '-' + str(index_validation)\n",
    "        df.loc[train_indecies,current_val_labels] = TRAIN\n",
    "        df.loc[val_indecies,current_val_labels] = VALIDATION\n",
    "        df.loc[test_set_indecies,current_val_labels] = TEST\n",
    "        \n",
    "        # Update the index for labeling. Starting from 0 is expected easier for the later accessing data for training.\n",
    "        index_validation = index_validation + 1\n",
    "    \n",
    "    # Output the results.\n",
    "    df_output = df.copy()\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927584c",
   "metadata": {},
   "source": [
    "## Run in Batch\n",
    "\n",
    "This part I extract raw data and extract features in batch from all collected data.\n",
    "\n",
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aaf0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My dumb verion of finding a list's member exisiting in a string or not, an alternative could be: https://thispointer.com/combine-for-loop-if-else-statement-in-python/\n",
    "def check_string(input_string, target_lists):\n",
    "    for x in target_lists:\n",
    "        print(x)\n",
    "        if x in input_string:\n",
    "            output_string = x\n",
    "            break\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e99f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new folder to store this batch of calculation results.\n",
    "def output_csv(args, df_input):\n",
    "    dirpath_results = args.results_save_path\n",
    "    now = datetime.datetime.now()\n",
    "    datestamp = now.strftime(\"%d-%m-%H-%M\")\n",
    "    results_folder_path = dirpath_results + datestamp + '/'\n",
    "\n",
    "    if os.path.exists(results_folder_path) is False:\n",
    "        os.makedirs(results_folder_path)\n",
    "\n",
    "    # Write the current dataframe as a csv into the new created folder.\n",
    "    df_input.to_csv(results_folder_path + args.results_csv_filename, encoding='utf-8', index=False, header=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2b21b",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d2f6a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part reads data file-wisely and generate designed features, and run the pipeline.\n",
    "def run(args):\n",
    "    # Initialize dataframes and storing lists.\n",
    "    FEATURES_SET = [LUX, LABELS, PID]\n",
    "    df_pre_features = pd.DataFrame(columns=FEATURES_SET)\n",
    "    frames_pre_features = []\n",
    "    \n",
    "    # Start to read raw data from files.\n",
    "    raw_data_path = args.path\n",
    "    # List all directory names\n",
    "    dirs_list = []\n",
    "    for (dir_path, dir_names, file_names) in walk(raw_data_path):\n",
    "        dirs_list.extend(dir_names)\n",
    "        break\n",
    "    \n",
    "    # Traverse all the file names in a given directory.\n",
    "    for dir_name in dirs_list:\n",
    "        dir_path = raw_data_path + dir_name + '/'\n",
    "        file_names_list = []\n",
    "        df_get_labels = pd.DataFrame()\n",
    "        df_current_trial = pd.DataFrame(columns=FEATURES_SET)\n",
    "\n",
    "        for (_, _, file_names) in walk(dir_path):\n",
    "            file_names_list.extend(file_names)\n",
    "\n",
    "        # Find the targetted files.\n",
    "        for file_name in file_names:\n",
    "            if TWODMODE in file_name:\n",
    "                if LEFT in file_name:\n",
    "                    file_path_left = dir_path + file_name\n",
    "                elif RIGHT in file_name:\n",
    "                    file_path_right = dir_path + file_name\n",
    "\n",
    "        # Step 1: Synchronize 2 eyes' targetted data (both remove the 'default' ones).\n",
    "        df_sync = upsample_timestamps_sync(left_eye_file_path=file_path_left, right_eye_file_path=file_path_right)\n",
    "\n",
    "        # Step 2: Deblink, and interpolate.\n",
    "        # Process the left eye data.\n",
    "        df_deblink = deblinks(df_input=df_sync, confidence_column_label=LCF, diameter_column_label=LDM, isblink_column_label=ISBLINK_LEFT)\n",
    "        # Process the right eye data.\n",
    "        df_deblink = deblinks(df_input=df_deblink, confidence_column_label=RCF, diameter_column_label=RDM, isblink_column_label=ISBLINK_RIGHT)\n",
    "\n",
    "        # Step 3: Artifect rejection\n",
    "        df_rej = rej_artifact(df_input=df_deblink, diameter_column_label=LDM)\n",
    "        # Process the right eye data.\n",
    "        df_rej = rej_artifact(df_input=df_rej, diameter_column_label=RDM)\n",
    "\n",
    "        # Step 4: Segment data using an overlapping time window.\n",
    "        windows = segment(args=args, df_input=df_rej)\n",
    "\n",
    "        # Step 5: Get labels and experiment conditions.\n",
    "        # Assign features into the current trial's dataframe.\n",
    "        df_get_labels = df_rej.copy()\n",
    "        # Lux conditions.\n",
    "        lux_condition = [iterator for iterator in LUXS_SET if iterator in dir_name][0] # My learnt version from: https://thispointer.com/combine-for-loop-if-else-statement-in-python/\n",
    "        df_get_labels.loc[:, LUX] = lux_condition\n",
    "        df_get_labels = df_get_labels.copy()\n",
    "        # Task difficulty labels.\n",
    "        task_diff_condition = [iterator for iterator in TASKDIFFS_SET if iterator in dir_name][0]\n",
    "        df_get_labels.loc[:, LABEL] = task_diff_condition\n",
    "        df_get_labels = df_get_labels.copy()\n",
    "        # PID.\n",
    "        pid = dir_name.split('-')[-1]\n",
    "        df_get_labels.loc[:, PID] = pid\n",
    "        df_get_labels = df_get_labels.copy()\n",
    "        \n",
    "        # Step 6: Generate wavelet features.And re-set instances: vertically decrease but horizontally increase.\n",
    "        df_freq_analyzed = freq_analysis(args=args, df_input=df_get_labels, windows_indices=windows)\n",
    "        \n",
    "        # Step 7: Split the dataset into traing, validation, and testing datasets.\n",
    "        df_split = time_series_cross_validation(args=args, df_input=df_freq_analyzed)\n",
    "    \n",
    "    #     # Debug Area: Plot to pre-view pupil data. TODO: delete later, for debugging and preprocessing validation: is there any problems?\n",
    "#         break\n",
    "    #     plot_pupil_diameters(df_input=df_sync, title_label=dir_name + ' synchronized')\n",
    "    #     plot_pupil_diameters(df_input=df_deblink, title_label=dir_name + ' delinked')\n",
    "    #     plot_pupil_diameters(df_input=df_rej, title_label=dir_name + ' rejected artifact')\n",
    "        \n",
    "        # Wrap up.\n",
    "        df_current_trial = df_split.copy()\n",
    "        frames_pre_features.append(df_current_trial)\n",
    "\n",
    "    # Merge dataframes/single trials' data into a large dataset.\n",
    "    df_pre_features = pd.concat(frames_pre_features)\n",
    "    \n",
    "    # Output the results.\n",
    "    df_output = df_pre_features.copy()\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561d57c",
   "metadata": {},
   "source": [
    "### Implementation - My Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "969281d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argument allocations.\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-f')\n",
    "    parser.add_argument(\"--path\", type=str, default='../Data/RawData4ML/VersionNovAll/', \n",
    "                        help=\"The path to read raw pupil diameter data.\")\n",
    "    parser.add_argument('--using_cuda', type=bool, default=True,\n",
    "                        help='cuda/cpu')\n",
    "    parser.add_argument('--gpu_ids', type=bool, default=[0],\n",
    "                        help='cuda/cpu')\n",
    "    parser.add_argument('--argument_save_path', type=str, default='../Data/Results/',\n",
    "                        help='The path to save arguments.')\n",
    "    parser.add_argument('--results_save_path', type=str, default='../Data/Results/',\n",
    "                    help='The path to save processed data and generated features.')\n",
    "    parser.add_argument('--results_csv_filename', type=str, default='results.csv',\n",
    "                    help='The result csv filename.')\n",
    "    parser.add_argument('--time_window_length', type=float, default=4.0,\n",
    "                        help='The length of time window unitted in second.')\n",
    "    parser.add_argument('--overlap_length', type=float, default=3.0,\n",
    "                        help='The length of overlapped time window unitted in second.')\n",
    "    parser.add_argument('--test_pct', type=float, default=0.1,\n",
    "                        help='The percentage of the testing set of the whole dataset.')\n",
    "    parser.add_argument('--val_pct', type=float, default=0.1,\n",
    "                        help='The percentage of the validation set of the whole dataset.') # Hence that the traing:validation:testing = (1-test_pct-val_pct):val_pct:test_pct.\n",
    "    return parser.parse_args(args = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad65a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement.\n",
    "# Get arguments.\n",
    "args = parse_args()\n",
    "df_features = run(args=args)\n",
    "output_csv(args=args, df_input=df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b837b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

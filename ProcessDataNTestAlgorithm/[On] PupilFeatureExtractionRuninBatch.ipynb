{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537937a8",
   "metadata": {},
   "source": [
    "# The Pupil Features Extracted in Batch Pipeline.\n",
    "## Introuduction\n",
    "The features to be extracted includes blinking rate and chunks of wavelet coefficients.\n",
    "\n",
    "## Reference\n",
    "1. The previous codes.https://github.com/BaiYunpeng1949/MobileEyeComputing/tree/master/ProcessDataNTestAlgorithm\n",
    "2. TODO: add papers here.\n",
    "3. My work: https://docs.google.com/document/d/1oLv3oJQLjst1_pYgd_UA3RRL1fRGSbZ6uvlMuxmZR2k/edit#heading=h.r01ccf7ox05g\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75874ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "import pywt\n",
    "from numba import jit\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62025f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(ax, data1, data2, param_dict):\n",
    "    out = ax.plot(data1, data2, **param_dict)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c74efe",
   "metadata": {},
   "source": [
    "## File-wise/Task-wise Data Pre-processing\n",
    "### [Suspended] Left and Right Eye Synchronization and Calculate the Difference\n",
    "Since I now implement ML/AI models to do a more inclusive analysis where more features could be analyzed, I will use the feature of the difference between left and right eye data.\n",
    "\n",
    "\n",
    "1. Referenced from the previous work located as: https://github.com/BaiYunpeng1949/MobileEyeComputing/blob/master/ProcessDataNTestAlgorithm/LeftRightEyesSyncData.ipynb\n",
    "\n",
    "2. The difference between two eyes were found correlated with cognitive workload estimation as well, see the related paper: Optimizing the usage of pupillary based indicators for cognitive workload. Reading link: https://docs.google.com/document/d/1jBezc9kqaziGlWk6sSgCvyjHTlpD5G6OoNZyh7K2HIo/edit#heading=h.qqfv1ot6zjd8\n",
    "\n",
    "Besides, there is a considerable difference in pupil size variation for right and left eyes of the participants. See the paper: Exploring pupil size variation as a cognitive load indicator in visualization studies, link: https://drive.google.com/file/d/1z8O1NGNYA87La-CVMSr7cQkJ-VVOOUSe/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312ae7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pre_dtw_sync(left_eye_file_path, right_eye_file_path):   \n",
    "    # Configure the parameters\n",
    "    entity_dwt_align = 'Timestamp'\n",
    "    entity_dwt_apply_dia = 'Diameter'\n",
    "    entity_dwt_apply_conf = 'Confidence'\n",
    "    left_eye = 'left'\n",
    "    right_eye = 'right'\n",
    "    \n",
    "    # Tag labels\n",
    "    SAMPLING_RATE_LEFT = int((left_eye_file_path.split('_')[-1]).split('Hz')[0])\n",
    "    data_left = pd.read_csv(left_eye_file_path)\n",
    "\n",
    "    SAMPLING_RATE_RIGHT = int((right_eye_file_path.split('_')[-1]).split('Hz')[0])\n",
    "    data_right = pd.read_csv(right_eye_file_path)\n",
    "    \n",
    "    df_left = data_left[['Timestamp','Confidence','Diameter','Event']].copy()\n",
    "    df_right = data_right[['Timestamp','Confidence','Diameter','Event']].copy()\n",
    "    \n",
    "    # Determine the left and right eye data's size first: which one is bigger?\n",
    "    # Identify the number of elements in the left/right eyes data. If one applies the up-sampling method, the larger eye needs to be put in the first argument.\n",
    "    len_left = len(df_left)\n",
    "    len_right = len(df_right)\n",
    "    if len_left >= len_right:\n",
    "        df_reference = df_left.copy()  # df_reference: the one that being put in the first argument, as a reference.\n",
    "        df_alignment = df_right.copy() # df_alignment: the one that being put in the second argument, to be aligned to the reference.\n",
    "        df_origin = df_left.copy()\n",
    "        SR_SYNC = SAMPLING_RATE_LEFT\n",
    "    elif len_left < len_right:\n",
    "        df_reference = df_right.copy()\n",
    "        df_alignment = df_left.copy()\n",
    "        df_origin = df_right.copy()\n",
    "        SR_SYNC = SAMPLING_RATE_RIGHT\n",
    "    \n",
    "    # Calculate for warping.\n",
    "    distance, path = fastdtw(df_reference[entity_dwt_align], df_alignment[entity_dwt_align], dist=euclidean)\n",
    "    \n",
    "    return path, df_reference, df_alignment, entity_dwt_apply_dia, entity_dwt_apply_conf, df_origin, SR_SYNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab963e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function scnchronize and align/merge 2 eyes' numerical values, including diamter and confidence values.\n",
    "# The method of getting the average/mean value of 2 eyes' data as computing target is referenced from the mention in LHIPA.\n",
    "def _dtw_synchronize_merge_data(path_dwt, df_reference, df_alignment, entity_dwt_apply):\n",
    "    # Synchronize\n",
    "    data_sync = []\n",
    "    for i in range(0, len(path_dwt)):\n",
    "        data_sync.append([path_dwt[i][0],  # The index column is for dropping out duplicates.\n",
    "                         df_reference[entity_dwt_apply].iloc[path_dwt[i][0]],\n",
    "                         df_alignment[entity_dwt_apply].iloc[path_dwt[i][1]]])\n",
    "    df_sync = pd.DataFrame(data=data_sync,\n",
    "                           columns=['Index', \n",
    "                                    'Reference '+entity_dwt_apply, \n",
    "                                    'Alignment '+entity_dwt_apply]).dropna()\n",
    "    df_sync = df_sync.drop_duplicates(subset=['Index']) # Drop the duplicates according to the index of the reference.\n",
    "    df_sync = df_sync.reset_index(drop=True)\n",
    "    # Merge/Align\n",
    "    df_sync['Avg'+entity_dwt_apply] = df_sync.loc[:, ['Reference '+entity_dwt_apply, 'Alignment '+entity_dwt_apply]].mean(axis = 1)\n",
    "    # Calculate the difference\n",
    "    df_sync['Diff'+entity_dwt_apply] = df_sync['Reference '+entity_dwt_apply] - df_sync['Alignment '+entity_dwt_apply]\n",
    "    \n",
    "    return df_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b321f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronize the given 2 eyes' data.\n",
    "def dtw_sync(left_eye_file_path, right_eye_file_path):\n",
    "    # Prepare for the sychronization.\n",
    "    path, df_reference, df_alignment, entity_dwt_apply_dia, entity_dwt_apply_conf, df_origin, SR_SYNC = _pre_dtw_sync(left_eye_file_path = left_eye_file_path, \n",
    "                                                                                                                  right_eye_file_path = right_eye_file_path)\n",
    "\n",
    "    \n",
    "    # Synchronize, merge, and label data.\n",
    "    df_sync_dia = _dtw_synchronize_merge_data(path_dwt=path,\n",
    "                                              df_reference=df_reference,\n",
    "                                              df_alignment=df_alignment,\n",
    "                                              entity_dwt_apply=entity_dwt_apply_dia)\n",
    "    df_sync_conf = _dtw_synchronize_merge_data(path_dwt=path,\n",
    "                                               df_reference=df_reference,\n",
    "                                               df_alignment=df_alignment,\n",
    "                                               entity_dwt_apply=entity_dwt_apply_conf)\n",
    "    \n",
    "    # Integrate into one dataframe.\n",
    "    df_sync = pd.DataFrame()\n",
    "    df_sync['Timestamp'] = df_origin['Timestamp']\n",
    "    df_sync['Confidence'] = df_sync_conf['AvgConfidence']\n",
    "    df_sync['Diameter'] = df_sync_dia['AvgDiameter']\n",
    "    df_sync['Event'] = df_origin['Event']\n",
    "    df_sync['DiffDiameter'] = df_sync_dia['DiffDiameter']\n",
    "    \n",
    "    # Output and save into a csv file.\n",
    "    df_export = df_sync.copy()\n",
    "    file_name = left_eye_file_path.split('/')[-2:]\n",
    "\n",
    "    folder_path = '../Data/PreprocessedData/' + file_name[0] + '/'\n",
    "    if os.path.exists(folder_path) is False:\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    write_file_name = 'synchronized_' + str(SR_SYNC) + 'Hz.csv'\n",
    "    write_file_path = folder_path + write_file_name\n",
    "    df_export.to_csv(write_file_path)\n",
    "    \n",
    "    return df_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896c170",
   "metadata": {},
   "source": [
    "### Merge Two Eyes' Data using Timestamps, Interpolate, and Calculate the Difference\n",
    "Reference: a blog introducing 2 sensors' data fusion - https://stackoverflow.com/questions/14079766/synchronize-dataset-multiple-users-multiple-timestamps\n",
    "\n",
    "#### Configuration Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b7c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TS = 'Timestamp'\n",
    "CF = 'Confidence'\n",
    "DM = 'Diameter'\n",
    "LDM = 'Left Diameter'\n",
    "RDM = 'Right Diameter'\n",
    "LCF = 'Left Confidence'\n",
    "RCF = 'Right Confidence'\n",
    "AVEDM = 'Average Diameter'\n",
    "AVECF = 'Average Confidence'\n",
    "DIFFDM = 'Difference Diameter'  # By my default: Left - Right - unit in pixels.\n",
    "SR = 'Averaged Sampling Rate'\n",
    "EVENT = 'Event'\n",
    "TARGET_TASK = 'sitting'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a4a93",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "To be noted that, when using the interpolation, we need to avoid overshooting, which is easily caused by spline curves. Otherwise we will have plenty negative pupil diameters and large outliers. I would use the 'linear' method to interpolate as suggested by Marshall. Apart from that, one might also want to try [monotonic](https://stackoverflow.com/questions/40072420/interpolate-without-having-negative-values-in-python) interpolators to avoid overshootings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9779287",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERPOLATE_TYPE = 'linear'\n",
    "LIMIT_DIRECTION = 'both'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eed6af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_timestamps_sync(left_eye_file_path, right_eye_file_path):\n",
    "    # Read data from csv into dataframes.\n",
    "    df_left = pd.read_csv(left_eye_file_path)\n",
    "    df_right = pd.read_csv(right_eye_file_path)\n",
    "    df_left = df_left[[TS,CF,DM,EVENT]].copy()\n",
    "    df_right = df_right[[TS,CF,DM,EVENT]].copy()\n",
    "\n",
    "    # Collect data from the targetted events: sitting.\n",
    "    df_left = df_left.loc[df_left[EVENT] == TARGET_TASK]\n",
    "    df_right = df_right.loc[df_right[EVENT] == TARGET_TASK]\n",
    "#     print(len(df_left[df_left[EVENT]=='default']))\n",
    "#     print(len(df_right[df_right[EVENT]=='default']))\n",
    "    \n",
    "    # Get the diameter data indexed by timestamps.\n",
    "    left_diameters = df_left[DM].to_numpy() \n",
    "    series_left = pd.Series(left_diameters, index=df_left[TS])\n",
    "    right_diameters = df_right[DM].to_numpy() \n",
    "    series_right = pd.Series(right_diameters, index=df_right[TS])\n",
    "    \n",
    "    # Synchronize 2 eyes' data by listing all timestamps, this process is actually a up-sampling.\n",
    "    df_sync = pd.DataFrame([series_left, series_right]).T.sort_index()\n",
    "    df_sync = df_sync.rename(columns={0: LDM, 1: RDM})\n",
    "\n",
    "    #Interpolate all the NAN values using 'Spline 3' method with a bi-directional strategy to fill both the first and the last NAN values.\n",
    "    df_sync = df_sync.interpolate(method=INTERPOLATE_TYPE, limit_direction=LIMIT_DIRECTION, axis=0)\n",
    "#     df_sync = df_sync.ffill()\n",
    "    \n",
    "    # Align the confidence values according to the timestamps. \n",
    "    # Reference: Adding values in new column based on indexes with pandas in python. https://stackoverflow.com/questions/45636105/adding-values-in-new-column-based-on-indexes-with-pandas-in-python\n",
    "    df_sync[LCF] = df_sync.index.to_series().map(df_left.set_index(TS)[CF])\n",
    "    df_sync[RCF] = df_sync.index.to_series().map(df_right.set_index(TS)[CF])\n",
    "    \n",
    "    # Interpolate NAN values using the normal linear method.\n",
    "    df_sync[LCF] = df_sync[LCF].interpolate(method=INTERPOLATE_TYPE, limit_direction=LIMIT_DIRECTION, axis=0)\n",
    "    df_sync[RCF] = df_sync[RCF].interpolate(method=INTERPOLATE_TYPE, limit_direction=LIMIT_DIRECTION, axis=0)\n",
    "    \n",
    "#     # Get the difference and average of two eyes' diameter data and confidence values.\n",
    "#     df_sync[AVEDM] = (df_sync[LDM] + df_sync[RDM]) / 2\n",
    "#     df_sync[DIFFDM] = df_sync[LDM] - df_sync[RDM]\n",
    "#     df_sync[AVECF] = (df_sync[LCF] + df_sync[RCF]) / 2\n",
    "    \n",
    "    # Get a new column storing the current trial's averaged sampling rate (the up-sampled version by interpolation).\n",
    "    ave_samp_rate = len(df_sync) / (df_sync.index[-1] - df_sync.index[0])\n",
    "    df_sync.loc[:, SR] = ave_samp_rate\n",
    "    df_sync = df_sync.copy()\n",
    "    \n",
    "    return df_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df6e65",
   "metadata": {},
   "source": [
    "### Deblinks and Blinking Rate Extraction\n",
    "\n",
    "Reference: Check how David Linderbaure's group deal with the blinks. Use confidence to identify blinks. They removed the data within 200ms. However, they did not interpolate the eliminated ones. Here I clean data before and after 200ms of blinks. The input is the numpy data list of the \"confidence\" conlumn. Then return a list that marks which indecies are blinks. \n",
    "\n",
    "\n",
    "#### Configuration Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0fdfce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deblink.\n",
    "BLINK_EXTENSION_TIMEWINDOW = 0.2 # 200ms\n",
    "MIN_CF = 0.25\n",
    "MIN_NUM_SAMPLE_BLINKS = 2\n",
    "\n",
    "# Smooth.\n",
    "WIN_TYPE = 'hann'\n",
    "HANN_WINDOW_SIZE = 5\n",
    "\n",
    "# Interpolate.\n",
    "CURVE_TYPE = 'linear'\n",
    "CURVE_ORDER = 3\n",
    "\n",
    "# Column configuration.\n",
    "ISBLINK = 'isBlink'\n",
    "ISBLINK_LEFT = 'isBlink-Left'\n",
    "ISBLINK_RIGHT = 'isBlink-Right'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734bcb1",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12eec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deblinks(df_input, confidence_column_label, diameter_column_label, isblink_column_label):\n",
    "    # Feed in the dataframe to be processed and specific columns of the same eye/the averaged eye.\n",
    "    # To be noted that the input was indexed by the timestamps. \n",
    "    # One Has to use the form of df_input[Col][df_input.index[i]] to reach the ith element.\n",
    "    df = df_input.copy()\n",
    "    # Initiate all 0 values to the IsBlink column.\n",
    "    df.loc[:,isblink_column_label] = 0\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Parameter initilization\n",
    "    blinks = []\n",
    "    num_samples = len(df)\n",
    "    i = 0 # The index starter.\n",
    "    \n",
    "    # Identify the blinks according to the low confidence values.\n",
    "    while i < num_samples:\n",
    "        if df[confidence_column_label][df.index[i]] < MIN_CF and i < num_samples -1:\n",
    "            offset = 1\n",
    "            next_data = df[confidence_column_label][df.index[i+offset]]\n",
    "            while next_data < MIN_CF:\n",
    "                offset = offset + 1\n",
    "                if i + offset >= (num_samples - 1): # Check wheter exceeding the indecies boundary.\n",
    "                    break\n",
    "                next_data = df[confidence_column_label][df.index[i+offset]]\n",
    "            \n",
    "            # Judge whether the current index exceeds the 200ms time window.\n",
    "            if offset >= MIN_NUM_SAMPLE_BLINKS:\n",
    "                blinks.append((i, offset))\n",
    "            \n",
    "            i = i + offset\n",
    "        else:\n",
    "            i = i + 1\n",
    "    \n",
    "    # Mark data before and after BLINK_EXTENSION_TIMEWINDOW of samples.\n",
    "    for j in range(len(blinks)):\n",
    "        blink_index = blinks[j][0]\n",
    "        blink_length = blinks[j][1]\n",
    "        \n",
    "        # Mark blinks within the searched area as np.nan values.\n",
    "        for j in range(0, blink_length):\n",
    "            df[diameter_column_label][df.index[blink_index + j]] = np.nan # Flag an NAN for the blinkings' diameters.\n",
    "            df[isblink_column_label][df.index[blink_index + j]] = 1 # Flag a numerical value 1 for the blinkings.\n",
    "        \n",
    "        # Search for the time window with a length of 200ms. Then also mark blinks with np.nan values for the convenience of interplating.\n",
    "        # Decremnenting.\n",
    "        blink_start_timestamp = df.index[blink_index]\n",
    "        k_dec = 0\n",
    "        decrement_index = blink_index - k_dec\n",
    "        # Controlled by the boundary conditions.\n",
    "        while decrement_index >= 0:\n",
    "            dec_timestamp = df.index[decrement_index]\n",
    "            if blink_start_timestamp - dec_timestamp >= BLINK_EXTENSION_TIMEWINDOW:\n",
    "                break\n",
    "            else:\n",
    "                df[diameter_column_label][df.index[decrement_index]] = np.nan # Set an NAN flag for data processing on the blinking data.\n",
    "                df[isblink_column_label][df.index[decrement_index]] = 1 # Set an numerical flag on the blinking data.\n",
    "                k_dec = k_dec + 1\n",
    "                decrement_index = blink_index - k_dec\n",
    "        \n",
    "        # Incrementing - check the boundary limits first.\n",
    "        blink_stop_timestamp = df.index[blink_index + blink_length]\n",
    "        k_inc = 0\n",
    "        increment_index = blink_index + blink_length + k_inc\n",
    "        while increment_index < num_samples:\n",
    "            inc_timestamp = df.index[increment_index]\n",
    "            if inc_timestamp - blink_stop_timestamp >= BLINK_EXTENSION_TIMEWINDOW:\n",
    "                break\n",
    "            else:\n",
    "                df[diameter_column_label][df.index[increment_index]] = np.nan # Set an NAN flag for data processing on the blinking data.\n",
    "                df[isblink_column_label][df.index[increment_index]] = 1 # Set an numerical flag on the blinking data.\n",
    "                k_inc = k_inc + 1\n",
    "                increment_index = blink_index + blink_length + k_inc\n",
    "    \n",
    "    \n",
    "    # Smooth the data - [Suspended] - Since the objective of freqeuncy-based analysis was to detect singularities, smooth should not be included here.\n",
    "    # Besides, in Marshall's patent, https://patentimages.storage.googleapis.com/91/2f/5f/236d6711dcf6b6/US6090051.pdf, smooth was not applied.\n",
    "#     df[diameter_column_label] = df[diameter_column_label].rolling(window=HANN_WINDOW_SIZE, center=True, win_type=WIN_TYPE).mean()\n",
    "    \n",
    "    # Interpolate the data - included in Marshall's patent - however the interpolation would introduce a lot of large negative values.\n",
    "    df[diameter_column_label] = df[diameter_column_label].interpolate(method=CURVE_TYPE,order=CURVE_ORDER, limit_direction='both', axis=0)\n",
    "    \n",
    "    df_output = df.copy()\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87d550",
   "metadata": {},
   "source": [
    "### Artefact Rejection\n",
    "\n",
    "#### Configuration Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9276a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAMPLE_WIN_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a9b76",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "900859ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This part is directly cited from Sam's work.\n",
    "\n",
    "# Filtering outliers \n",
    "# Lan et al. 2020 - median filter with sliding window of 10s\n",
    "# Testing with numba optimised for-loop implementation of a Hampel Filter\n",
    "# Note to self: I think this filter is also commonly used for pupil diameter filtering\n",
    "@jit(nopython=True)\n",
    "def hampel_filter_forloop_numba(input_series, window_size, n_sigmas=3):\n",
    "    \n",
    "    n = len(input_series)\n",
    "    new_series = input_series.copy()\n",
    "    k = 1.4826 # scale factor for Gaussian distribution\n",
    "    indices = []\n",
    "    \n",
    "    for i in range((window_size),(n - window_size)):\n",
    "        x0 = np.nanmedian(input_series[(i - window_size):(i + window_size)])\n",
    "        S0 = k * np.nanmedian(np.abs(input_series[(i - window_size):(i + window_size)] - x0))\n",
    "        if (np.abs(input_series[i] - x0) > n_sigmas * S0):\n",
    "            new_series[i] = x0\n",
    "            indices.append(i)\n",
    "    \n",
    "    return new_series, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ad3e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rej_artifact(df_input, diameter_column_label):\n",
    "    df = df_input.copy()\n",
    "    x_, outlier_x_ = hampel_filter_forloop_numba(df[diameter_column_label].to_numpy(), HAMPLE_WIN_SIZE)\n",
    "    df[diameter_column_label] = x_.tolist()\n",
    "    df_output = df.copy()\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5a8af8",
   "metadata": {},
   "source": [
    "## Time-series Data Visualization\n",
    "In this part, I visualize pupil data for different cognitive activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0433a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pupil_diameters(df_input, title_label):\n",
    "    fig, ax = plt.subplots()\n",
    "    df_input[LDM].plot(ax=ax)  # Plot the left diameter data.\n",
    "    df_input[RDM].plot(ax=ax, title='Clean and Preprocessed Pupil Diameters (in pixels)\\n' + title_label)  # Plot the right diameter data.\n",
    "    ax.legend(['Left Eye', 'Right Eye'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155736b7",
   "metadata": {},
   "source": [
    "## Segmenting Time Windows\n",
    "\n",
    "In this part, I will use overlapped sliding windows to extract wavelet-related features.\n",
    "\n",
    "The reference publications include:\n",
    "1. [WiStress: Contactless Stress Monitoring Using Wireless Signals, 2021](https://dl.acm.org/doi/pdf/10.1145/3478121).\n",
    "2. [Feature extraction for robust physical activity recognition, 2017](https://hcis-journal.springeropen.com/articles/10.1186/s13673-017-0097-2).\n",
    "3. [Feature Engineering on Time-Series Data for Human Activity Recognition](https://towardsdatascience.com/feature-engineering-on-time-series-data-transforming-signal-data-of-a-smartphone-accelerometer-for-72cbe34b8a60).\n",
    "4. [Indexing Cognitive Workload Based on Pupillary Response under Luminance and Emotional Changes, 2013](https://dl-acm-org.libproxy1.nus.edu.sg/doi/pdf/10.1145/2449396.2449428).\n",
    "\n",
    "### Configuration Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "586e6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter configuration according to arguments from the functions we called.\n",
    "SAMPLING_RATE = 120 # The unit is Hertz.\n",
    "time_window_length = 5 # The unit is second.\n",
    "overlapping_length = 3 # The unit is second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f4191",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "The time window segmentation will be conducted file-wisely, i.e., each trial's data produces their own windows. Hence that to be aligned to the data pre-processing's file-wise fashion.\n",
    "\n",
    "By slicing data with overlapped sliding windows, the new instances will be created. The former instances are sample points, now is conjoint sliding window waiting to be transformed into multiple features described by wavelet decomposition.\n",
    "\n",
    "From here, the sample points are regarded as averaged ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6cbd20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I constructed by own overlapped sliding window here.\n",
    "def segment(df_input, window_length=time_window_length, overlap_length=overlapping_length):\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    # Parameter initialization.\n",
    "    num_samples = len(df)\n",
    "    i=0 # Store the sliding window's starting point.\n",
    "    offset_next = 0 # Store the next sliding window's starting index.\n",
    "    windows = []\n",
    "    \n",
    "    window_num = SAMPLING_RATE * window_length\n",
    "    overlap_num = SAMPLING_RATE * overlap_length\n",
    "    step_num = window_num - overlap_num\n",
    "    \n",
    "    # Determine the number of sample points according to the standard sampling rate, i.e., 120 Hz.\n",
    "    indices = np.arange(num_samples)\n",
    "    shape = (indices.size - window_num + 1, window_num)\n",
    "    stride = indices.strides * 2\n",
    "    view = np.lib.stride_tricks.as_strided(indices, strides = stride, shape = shape)[0::step_num]\n",
    "    output_2D_list = view.copy()\n",
    "    for i in range(len(output_2D_list)):\n",
    "        windows.append((output_2D_list[i][0], output_2D_list[i][-1]))\n",
    "    \n",
    "#     # Determine the number of sample points according to given sampling rate according to the timestamps.\n",
    "#     # I suspend this part, then use the standard data sampling part is for union data points and union feature expansion while using wavelet decomposition.\n",
    "#     windows = []\n",
    "#     while i < (num_samples - 1):\n",
    "#         starting_timestamp = df.index[i]\n",
    "#         offset = 1\n",
    "#         stopping_timestamp = df.index[i+offset]\n",
    "#         while stopping_timestamp - starting_timestamp < window_length:\n",
    "#             # Check whether reaches the overlapping edge.\n",
    "#             if stopping_timestamp - starting_timestamp <= (window_length - overlap_length):\n",
    "#                 offset_next = offset\n",
    "#                 # Then it should stop\n",
    "            \n",
    "#             offset = offset + 1\n",
    "#             if i + offset >= (num_samples - 1): # Check wheter exceeding the indecies boundary.\n",
    "#                 break\n",
    "#             stopping_timestamp = df.index[i+offset]\n",
    "            \n",
    "#         # Store the starting index and offset index.\n",
    "#         windows.append((i, i+offset))\n",
    "        \n",
    "#         # Update the starting index.\n",
    "#         i = i + offset_next\n",
    "        \n",
    "#     # Iteratively check whether the last window is too short, if it is shorter thant the overlapped area, then discard it.\n",
    "#     while True:\n",
    "#         last_starting_index = df.index[windows[-1][0]]\n",
    "#         last_ending_index = df.index[windows[-1][1]-1]\n",
    "#         last_time_window_length = last_ending_index - last_starting_index\n",
    "#         if last_time_window_length <= overlap_length:\n",
    "#             del windows[-1]\n",
    "#         else\n",
    "#             break\n",
    "    \n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abbd51",
   "metadata": {},
   "source": [
    "## Wavelet Coefficient Extraction\n",
    "\n",
    "This part generates frequency features using wavelet analysis.\n",
    "\n",
    "And generate new instances, which is composed of several sample points from the time window.\n",
    "\n",
    "### Configuration Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e00a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet_decomposition_level = 2\n",
    "wavelet = 'sym16'\n",
    "\n",
    "ISBLINK_LEFT = 'isBlink-Left'\n",
    "ISBLINK_RIGHT = 'isBlink-Right'\n",
    "\n",
    "# Experimental conditions\n",
    "# Luminance.\n",
    "LUX = 'Luminance'\n",
    "# Task difficulty - labels.\n",
    "LABEL = 'Labels'\n",
    "PID = 'PID'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e93f4",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55274b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_analysis(df_input, windows_indices, j=wavelet_decomposition_level):\n",
    "    df = df_input.copy()\n",
    "    windows = windows_indices\n",
    "\n",
    "    # Parameter initialization.\n",
    "    freq_features_two_eyes = []\n",
    "    freq_features_left = []\n",
    "    freq_features_right = []\n",
    "    column_name_left = 'Left-'\n",
    "    column_name_right = 'Right-'\n",
    "    \n",
    "    blinking_rates_left = []\n",
    "    blinking_rates_right = []\n",
    "    luxes = []\n",
    "    labels = []\n",
    "    pids = []\n",
    "    \n",
    "    for i in range(len(windows)):\n",
    "        starting_index = windows[i][0]\n",
    "        ending_index = windows[i][1] + 1\n",
    "        \n",
    "        # Get the frequency features.\n",
    "        data_left = df[LDM].to_list()[starting_index:ending_index]\n",
    "        data_right = df[RDM].to_list()[starting_index:ending_index]\n",
    "        \n",
    "        (cA2_left, cD2, cD1) = pywt.wavedec(data_left, wavelet, 'per', level=j)\n",
    "        (cA2_right, cD2, cD1) = pywt.wavedec(data_right, wavelet, 'per', level=j)\n",
    "        \n",
    "        cA2_two_eyes = np.concatenate((cA2_left, cA2_right), axis=0)\n",
    "        \n",
    "        freq_features_left.append(cA2_left)\n",
    "        freq_features_right.append(cA2_right)\n",
    "        freq_features_two_eyes.append(cA2_two_eyes)\n",
    "        \n",
    "        # Get the blinking rate feature from both eyes. df[df['col'] == value\n",
    "        blinks_left = np.array(df[ISBLINK_LEFT].to_list()[starting_index:ending_index])\n",
    "        blinking_rate_left = (np.sum(blinks_left))/len(blinks_left)\n",
    "        blinking_rates_left.append(blinking_rate_left)\n",
    "        \n",
    "        blinks_right = np.array(df[ISBLINK_RIGHT].to_list()[starting_index:ending_index])\n",
    "        blinking_rate_right = (np.sum(blinks_right))/len(blinks_right)\n",
    "        blinking_rates_right.append(blinking_rate_right)\n",
    "        \n",
    "        # Get the luminance feature. df['City'].iat[0]\n",
    "        luxes.append(df[LUX].iat[0])\n",
    "        \n",
    "        # Get the PID.\n",
    "        pids.append(df[PID].iat[0])\n",
    "        \n",
    "        # Get the task label.\n",
    "        labels.append(df[LABEL].iat[0])\n",
    "        \n",
    "    # Add high dimension features into the dataframe. Set the columns\n",
    "    # From now, the instances are vertically conpacted by the sliding windows, but horizontally expaned.\n",
    "    # Frequency features.\n",
    "    horizontal_length_left = np.array(freq_features_left).shape[1]\n",
    "    horizontal_length_right = np.array(freq_features_right).shape[1]\n",
    "    left_features = []\n",
    "    for i in range(horizontal_length_left):\n",
    "        feature_name = column_name_left + str(i)\n",
    "        left_features.append(feature_name)\n",
    "        \n",
    "    right_features = []\n",
    "    for i in range(horizontal_length_right):\n",
    "        feature_name = column_name_right + str(i)\n",
    "        right_features.append(feature_name)\n",
    "    \n",
    "    feature_names = left_features + right_features\n",
    "    df_freq = pd.DataFrame(freq_features_two_eyes, columns=feature_names)\n",
    "    \n",
    "    # Blinking rate features.\n",
    "    df_freq[ISBLINK_LEFT] = blinking_rates_left\n",
    "    df_freq[ISBLINK_RIGHT] = blinking_rates_right\n",
    "    \n",
    "    # Luminance features.\n",
    "    df_freq[LUX] = luxes\n",
    "    \n",
    "    # PID.\n",
    "    df_freq[PID] = pids\n",
    "    \n",
    "    # Label.\n",
    "    df_freq[LABEL] = labels\n",
    "        \n",
    "    df_output = df_freq.copy()\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ecf5ec",
   "metadata": {},
   "source": [
    "## Run in Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d9664f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argument configuration.\n",
    "TWODMODE = '2D'\n",
    "LEFT = 'left'\n",
    "RIGHT = 'right'\n",
    "ISBLINK = 'isBlink'\n",
    "raw_data_path = '../Data/RawData4ML/VersionOctober/' # In this case, the dirpath is mypath, dirnames contains sub-folders's names I need, and no filenames since there is no files there.\n",
    "dir_features = '../Data/Results/'\n",
    "\n",
    "# Experimental conditions\n",
    "# Luminance.\n",
    "LUX = 'Luminance'\n",
    "LOW = 'lowlux'\n",
    "MID = 'middlelux'\n",
    "HIGH = 'highlux'\n",
    "LUXS_SET = [LOW, MID, HIGH]\n",
    "# Task difficulty - labels.\n",
    "LABEL = 'Labels'\n",
    "NOBACK = 'nothing'\n",
    "ONEBACK = 'ONEBACK'\n",
    "TWOBACK = 'TWOBACK'\n",
    "THREEBACK = 'THREEBACK'\n",
    "TASKDIFFS_SET = [NOBACK, ONEBACK, TWOBACK, THREEBACK]\n",
    "\n",
    "PID = 'PID'\n",
    "\n",
    "# Create a dataframe to store results details. TODO: to be modified to align to short time windows. Columns - features; Rows - short segmentations.\n",
    "FEATURES_SET = [LUX, LABEL, PID]\n",
    "df_pre_features = pd.DataFrame(columns=FEATURES_SET)\n",
    "frames_pre_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c75ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My dumb verion of finding a list's member exisiting in a string or not, an alternative could be: https://thispointer.com/combine-for-loop-if-else-statement-in-python/\n",
    "def check_string(input_string, target_lists):\n",
    "    for x in target_lists:\n",
    "        print(x)\n",
    "        if x in input_string:\n",
    "            output_string = x\n",
    "            break\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d7705e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all directory names\n",
    "dirs_list = []\n",
    "for (dir_path, dir_names, file_names) in walk(raw_data_path):\n",
    "    dirs_list.extend(dir_names)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7524cc79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 53s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Traverse all the file names in a given directory.\n",
    "for dir_name in dirs_list:\n",
    "    dir_path = raw_data_path + dir_name + '/'\n",
    "    file_names_list = []\n",
    "    df_get_labels = pd.DataFrame()\n",
    "    df_current_trial = pd.DataFrame(columns=FEATURES_SET)\n",
    "    \n",
    "    for (_, _, file_names) in walk(dir_path):\n",
    "        file_names_list.extend(file_names)\n",
    "    \n",
    "    # Find the targetted files.\n",
    "    for file_name in file_names:\n",
    "        if TWODMODE in file_name:\n",
    "            if LEFT in file_name:\n",
    "                file_path_left = dir_path + file_name\n",
    "            elif RIGHT in file_name:\n",
    "                file_path_right = dir_path + file_name\n",
    "      \n",
    "    # Step 1: Synchronize 2 eyes' data.\n",
    "    df_sync = upsample_timestamps_sync(left_eye_file_path=file_path_left, right_eye_file_path=file_path_right)\n",
    "    \n",
    "    # Step 2: Deblink, and interpolate.\n",
    "    # Process the left eye data.\n",
    "    df_deblink = deblinks(df_input=df_sync, confidence_column_label=LCF, diameter_column_label=LDM, isblink_column_label=ISBLINK_LEFT)\n",
    "    # Process the right eye data.\n",
    "    df_deblink = deblinks(df_input=df_deblink, confidence_column_label=RCF, diameter_column_label=RDM, isblink_column_label=ISBLINK_RIGHT)\n",
    "    \n",
    "    # Step3: Artifect rejection\n",
    "    df_rej = rej_artifact(df_input=df_deblink, diameter_column_label=LDM)\n",
    "    # Process the right eye data.\n",
    "    df_rej = rej_artifact(df_input=df_rej, diameter_column_label=RDM)\n",
    "    \n",
    "    # Step4: Segment data using an overlapping time window.\n",
    "    windows = segment(df_input=df_rej)\n",
    "    \n",
    "    # Step5: Get labels and experiment conditions.\n",
    "    # Assign features into the current trial's dataframe.\n",
    "    df_get_labels = df_rej.copy()\n",
    "    # Lux conditions.\n",
    "    lux_condition = [iterator for iterator in LUXS_SET if iterator in dir_name][0] # My learnt version from: https://thispointer.com/combine-for-loop-if-else-statement-in-python/\n",
    "    df_get_labels.loc[:, LUX] = lux_condition\n",
    "    df_get_labels = df_get_labels.copy()\n",
    "    # Task difficulty labels.\n",
    "    task_diff_condition = [iterator for iterator in TASKDIFFS_SET if iterator in dir_name][0]\n",
    "    df_get_labels.loc[:, LABEL] = task_diff_condition\n",
    "    df_get_labels = df_get_labels.copy()\n",
    "    # PID.\n",
    "    pid = dir_name.split('-')[-1]\n",
    "    df_get_labels.loc[:, PID] = pid\n",
    "    df_get_labels = df_get_labels.copy()\n",
    "    # Step6: Generate wavelet features.And re-set instances: vertically decrease but horizontally increase.\n",
    "    df_freq_analyzed = freq_analysis(df_input=df_get_labels, windows_indices=windows)\n",
    "    \n",
    "    df_current_trial = df_freq_analyzed.copy()\n",
    "    frames_pre_features.append(df_current_trial)\n",
    "#     break\n",
    "    \n",
    "#     # Debug Area: Plot to pre-view pupil data. TODO: delete later, for debugging and preprocessing validation: is there any problems?\n",
    "#     plot_pupil_diameters(df_input=df_sync, title_label=dir_name + ' synchronized')\n",
    "#     plot_pupil_diameters(df_input=df_deblink, title_label=dir_name + ' delinked')\n",
    "#     plot_pupil_diameters(df_input=df_rej, title_label=dir_name + ' rejected artifact')\n",
    "\n",
    "# Merge dataframes/single trials' data into a large dataset.\n",
    "df_pre_features = pd.concat(frames_pre_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6eb18b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left-0</th>\n",
       "      <th>Left-1</th>\n",
       "      <th>Left-2</th>\n",
       "      <th>Left-3</th>\n",
       "      <th>Left-4</th>\n",
       "      <th>Left-5</th>\n",
       "      <th>Left-6</th>\n",
       "      <th>Left-7</th>\n",
       "      <th>Left-8</th>\n",
       "      <th>Left-9</th>\n",
       "      <th>...</th>\n",
       "      <th>Right-145</th>\n",
       "      <th>Right-146</th>\n",
       "      <th>Right-147</th>\n",
       "      <th>Right-148</th>\n",
       "      <th>Right-149</th>\n",
       "      <th>isBlink-Left</th>\n",
       "      <th>isBlink-Right</th>\n",
       "      <th>Luminance</th>\n",
       "      <th>PID</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.878101</td>\n",
       "      <td>90.000122</td>\n",
       "      <td>89.429937</td>\n",
       "      <td>90.488841</td>\n",
       "      <td>90.425234</td>\n",
       "      <td>90.336186</td>\n",
       "      <td>90.534690</td>\n",
       "      <td>90.501258</td>\n",
       "      <td>90.304912</td>\n",
       "      <td>90.299277</td>\n",
       "      <td>...</td>\n",
       "      <td>94.908427</td>\n",
       "      <td>95.267926</td>\n",
       "      <td>94.824154</td>\n",
       "      <td>95.681502</td>\n",
       "      <td>95.550234</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>lowlux</td>\n",
       "      <td>07</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.272990</td>\n",
       "      <td>90.943095</td>\n",
       "      <td>90.475785</td>\n",
       "      <td>90.534682</td>\n",
       "      <td>90.372235</td>\n",
       "      <td>90.283447</td>\n",
       "      <td>90.175733</td>\n",
       "      <td>90.071457</td>\n",
       "      <td>89.966849</td>\n",
       "      <td>89.862239</td>\n",
       "      <td>...</td>\n",
       "      <td>96.455060</td>\n",
       "      <td>95.353432</td>\n",
       "      <td>94.965808</td>\n",
       "      <td>95.486500</td>\n",
       "      <td>94.833888</td>\n",
       "      <td>0.168333</td>\n",
       "      <td>0.203333</td>\n",
       "      <td>lowlux</td>\n",
       "      <td>07</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89.356322</td>\n",
       "      <td>88.406687</td>\n",
       "      <td>88.938648</td>\n",
       "      <td>89.101954</td>\n",
       "      <td>88.632723</td>\n",
       "      <td>89.213376</td>\n",
       "      <td>88.886822</td>\n",
       "      <td>88.681709</td>\n",
       "      <td>89.061931</td>\n",
       "      <td>88.684184</td>\n",
       "      <td>...</td>\n",
       "      <td>95.822918</td>\n",
       "      <td>96.589191</td>\n",
       "      <td>96.326834</td>\n",
       "      <td>96.544795</td>\n",
       "      <td>95.752655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>lowlux</td>\n",
       "      <td>07</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.591427</td>\n",
       "      <td>90.381350</td>\n",
       "      <td>89.822829</td>\n",
       "      <td>90.178344</td>\n",
       "      <td>90.201855</td>\n",
       "      <td>90.039328</td>\n",
       "      <td>89.327095</td>\n",
       "      <td>90.271126</td>\n",
       "      <td>89.445756</td>\n",
       "      <td>89.488081</td>\n",
       "      <td>...</td>\n",
       "      <td>93.682537</td>\n",
       "      <td>93.863888</td>\n",
       "      <td>94.325592</td>\n",
       "      <td>93.692027</td>\n",
       "      <td>93.015056</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>lowlux</td>\n",
       "      <td>07</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89.352206</td>\n",
       "      <td>89.354459</td>\n",
       "      <td>90.085756</td>\n",
       "      <td>90.154200</td>\n",
       "      <td>90.493916</td>\n",
       "      <td>88.939724</td>\n",
       "      <td>89.547730</td>\n",
       "      <td>90.279304</td>\n",
       "      <td>90.332414</td>\n",
       "      <td>91.536459</td>\n",
       "      <td>...</td>\n",
       "      <td>93.907159</td>\n",
       "      <td>92.912075</td>\n",
       "      <td>93.245723</td>\n",
       "      <td>93.272628</td>\n",
       "      <td>94.553329</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>lowlux</td>\n",
       "      <td>07</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>...</td>\n",
       "      <td>91.685748</td>\n",
       "      <td>91.797745</td>\n",
       "      <td>91.581850</td>\n",
       "      <td>91.573155</td>\n",
       "      <td>91.780403</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.431667</td>\n",
       "      <td>lowlux</td>\n",
       "      <td>02</td>\n",
       "      <td>THREEBACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>...</td>\n",
       "      <td>91.279524</td>\n",
       "      <td>91.298279</td>\n",
       "      <td>91.303723</td>\n",
       "      <td>91.315547</td>\n",
       "      <td>91.318091</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>lowlux</td>\n",
       "      <td>02</td>\n",
       "      <td>THREEBACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>...</td>\n",
       "      <td>91.266389</td>\n",
       "      <td>91.254105</td>\n",
       "      <td>91.241145</td>\n",
       "      <td>91.231029</td>\n",
       "      <td>91.214267</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663333</td>\n",
       "      <td>lowlux</td>\n",
       "      <td>02</td>\n",
       "      <td>THREEBACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>...</td>\n",
       "      <td>89.824706</td>\n",
       "      <td>89.854385</td>\n",
       "      <td>89.802735</td>\n",
       "      <td>90.034177</td>\n",
       "      <td>89.511720</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>lowlux</td>\n",
       "      <td>02</td>\n",
       "      <td>THREEBACK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>93.249352</td>\n",
       "      <td>...</td>\n",
       "      <td>89.672283</td>\n",
       "      <td>89.685439</td>\n",
       "      <td>89.653108</td>\n",
       "      <td>89.686458</td>\n",
       "      <td>89.527809</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.536667</td>\n",
       "      <td>lowlux</td>\n",
       "      <td>02</td>\n",
       "      <td>THREEBACK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2812 rows × 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Left-0     Left-1     Left-2     Left-3     Left-4     Left-5  \\\n",
       "0   90.878101  90.000122  89.429937  90.488841  90.425234  90.336186   \n",
       "1   90.272990  90.943095  90.475785  90.534682  90.372235  90.283447   \n",
       "2   89.356322  88.406687  88.938648  89.101954  88.632723  89.213376   \n",
       "3   89.591427  90.381350  89.822829  90.178344  90.201855  90.039328   \n",
       "4   89.352206  89.354459  90.085756  90.154200  90.493916  88.939724   \n",
       "..        ...        ...        ...        ...        ...        ...   \n",
       "78  93.249352  93.249352  93.249352  93.249352  93.249352  93.249352   \n",
       "79  93.249352  93.249352  93.249352  93.249352  93.249352  93.249352   \n",
       "80  93.249352  93.249352  93.249352  93.249352  93.249352  93.249352   \n",
       "81  93.249352  93.249352  93.249352  93.249352  93.249352  93.249352   \n",
       "82  93.249352  93.249352  93.249352  93.249352  93.249352  93.249352   \n",
       "\n",
       "       Left-6     Left-7     Left-8     Left-9  ...  Right-145  Right-146  \\\n",
       "0   90.534690  90.501258  90.304912  90.299277  ...  94.908427  95.267926   \n",
       "1   90.175733  90.071457  89.966849  89.862239  ...  96.455060  95.353432   \n",
       "2   88.886822  88.681709  89.061931  88.684184  ...  95.822918  96.589191   \n",
       "3   89.327095  90.271126  89.445756  89.488081  ...  93.682537  93.863888   \n",
       "4   89.547730  90.279304  90.332414  91.536459  ...  93.907159  92.912075   \n",
       "..        ...        ...        ...        ...  ...        ...        ...   \n",
       "78  93.249352  93.249352  93.249352  93.249352  ...  91.685748  91.797745   \n",
       "79  93.249352  93.249352  93.249352  93.249352  ...  91.279524  91.298279   \n",
       "80  93.249352  93.249352  93.249352  93.249352  ...  91.266389  91.254105   \n",
       "81  93.249352  93.249352  93.249352  93.249352  ...  89.824706  89.854385   \n",
       "82  93.249352  93.249352  93.249352  93.249352  ...  89.672283  89.685439   \n",
       "\n",
       "    Right-147  Right-148  Right-149  isBlink-Left  isBlink-Right  Luminance  \\\n",
       "0   94.824154  95.681502  95.550234      0.325000       0.370000     lowlux   \n",
       "1   94.965808  95.486500  94.833888      0.168333       0.203333     lowlux   \n",
       "2   96.326834  96.544795  95.752655      0.000000       0.000000     lowlux   \n",
       "3   94.325592  93.692027  93.015056      0.135000       0.195000     lowlux   \n",
       "4   93.245723  93.272628  94.553329      0.135000       0.195000     lowlux   \n",
       "..        ...        ...        ...           ...            ...        ...   \n",
       "78  91.581850  91.573155  91.780403      1.000000       0.431667     lowlux   \n",
       "79  91.303723  91.315547  91.318091      1.000000       0.530000     lowlux   \n",
       "80  91.241145  91.231029  91.214267      1.000000       0.663333     lowlux   \n",
       "81  89.802735  90.034177  89.511720      1.000000       0.816667     lowlux   \n",
       "82  89.653108  89.686458  89.527809      1.000000       0.536667     lowlux   \n",
       "\n",
       "    PID     Labels  \n",
       "0    07    nothing  \n",
       "1    07    nothing  \n",
       "2    07    nothing  \n",
       "3    07    nothing  \n",
       "4    07    nothing  \n",
       "..  ...        ...  \n",
       "78   02  THREEBACK  \n",
       "79   02  THREEBACK  \n",
       "80   02  THREEBACK  \n",
       "81   02  THREEBACK  \n",
       "82   02  THREEBACK  \n",
       "\n",
       "[2812 rows x 305 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf9c6f",
   "metadata": {},
   "source": [
    "## Output to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfdab6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new folder to store this batch of calculation results.\n",
    "dirpath_results = '../Data/Results/'\n",
    "now = datetime.datetime.now()\n",
    "datestamp = now.strftime(\"%d-%m-%H-%M\")\n",
    "results_folder_path = dirpath_results + datestamp + '/'\n",
    "\n",
    "if os.path.exists(results_folder_path) is False:\n",
    "    os.makedirs(results_folder_path)\n",
    "\n",
    "# Write the current dataframe as a csv into the new created folder.\n",
    "df_pre_features.to_csv(results_folder_path + 'results.csv', encoding='utf-8', index=False, header=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c307262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

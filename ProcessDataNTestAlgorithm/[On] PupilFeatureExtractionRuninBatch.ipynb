{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537937a8",
   "metadata": {},
   "source": [
    "# The Pupil Features Extracted in Batch Pipeline.\n",
    "## Introuduction\n",
    "The features to be extracted includes blinking rate and chunks of wavelet coefficients.\n",
    "\n",
    "## Reference\n",
    "1. The previous codes.https://github.com/BaiYunpeng1949/MobileEyeComputing/tree/master/ProcessDataNTestAlgorithm\n",
    "2. TODO: add papers here.\n",
    "3. My work: https://docs.google.com/document/d/1oLv3oJQLjst1_pYgd_UA3RRL1fRGSbZ6uvlMuxmZR2k/edit#heading=h.r01ccf7ox05g\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75874ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62025f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(ax, data1, data2, param_dict):\n",
    "    out = ax.plot(data1, data2, **param_dict)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c74efe",
   "metadata": {},
   "source": [
    "## File-wise/Task-wise Data Pre-processing\n",
    "### [Suspended] Left and Right Eye Synchronization and Calculate the Difference\n",
    "Since I now implement ML/AI models to do a more inclusive analysis where more features could be analyzed, I will use the feature of the difference between left and right eye data.\n",
    "\n",
    "\n",
    "1. Referenced from the previous work located as: https://github.com/BaiYunpeng1949/MobileEyeComputing/blob/master/ProcessDataNTestAlgorithm/LeftRightEyesSyncData.ipynb\n",
    "\n",
    "2. The difference between two eyes were found correlated with cognitive workload estimation as well, see the related paper: Optimizing the usage of pupillary based indicators for cognitive workload. Reading link: https://docs.google.com/document/d/1jBezc9kqaziGlWk6sSgCvyjHTlpD5G6OoNZyh7K2HIo/edit#heading=h.qqfv1ot6zjd8\n",
    "\n",
    "Besides, there is a considerable difference in pupil size variation for right and left eyes of the participants. See the paper: Exploring pupil size variation as a cognitive load indicator in visualization studies, link: https://drive.google.com/file/d/1z8O1NGNYA87La-CVMSr7cQkJ-VVOOUSe/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312ae7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pre_dtw_sync(left_eye_file_path, right_eye_file_path):   \n",
    "    # Configure the parameters\n",
    "    entity_dwt_align = 'Timestamp'\n",
    "    entity_dwt_apply_dia = 'Diameter'\n",
    "    entity_dwt_apply_conf = 'Confidence'\n",
    "    left_eye = 'left'\n",
    "    right_eye = 'right'\n",
    "    \n",
    "    # Tag labels\n",
    "    SAMPLING_RATE_LEFT = int((left_eye_file_path.split('_')[-1]).split('Hz')[0])\n",
    "    data_left = pd.read_csv(left_eye_file_path)\n",
    "\n",
    "    SAMPLING_RATE_RIGHT = int((right_eye_file_path.split('_')[-1]).split('Hz')[0])\n",
    "    data_right = pd.read_csv(right_eye_file_path)\n",
    "    \n",
    "    df_left = data_left[['Timestamp','Confidence','Diameter','Event']].copy()\n",
    "    df_right = data_right[['Timestamp','Confidence','Diameter','Event']].copy()\n",
    "    \n",
    "    # Determine the left and right eye data's size first: which one is bigger?\n",
    "    # Identify the number of elements in the left/right eyes data. If one applies the up-sampling method, the larger eye needs to be put in the first argument.\n",
    "    len_left = len(df_left)\n",
    "    len_right = len(df_right)\n",
    "    if len_left >= len_right:\n",
    "        df_reference = df_left.copy()  # df_reference: the one that being put in the first argument, as a reference.\n",
    "        df_alignment = df_right.copy() # df_alignment: the one that being put in the second argument, to be aligned to the reference.\n",
    "        df_origin = df_left.copy()\n",
    "        SR_SYNC = SAMPLING_RATE_LEFT\n",
    "    elif len_left < len_right:\n",
    "        df_reference = df_right.copy()\n",
    "        df_alignment = df_left.copy()\n",
    "        df_origin = df_right.copy()\n",
    "        SR_SYNC = SAMPLING_RATE_RIGHT\n",
    "    \n",
    "    # Calculate for warping.\n",
    "    distance, path = fastdtw(df_reference[entity_dwt_align], df_alignment[entity_dwt_align], dist=euclidean)\n",
    "    \n",
    "    return path, df_reference, df_alignment, entity_dwt_apply_dia, entity_dwt_apply_conf, df_origin, SR_SYNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab963e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function scnchronize and align/merge 2 eyes' numerical values, including diamter and confidence values.\n",
    "# The method of getting the average/mean value of 2 eyes' data as computing target is referenced from the mention in LHIPA.\n",
    "def _dtw_synchronize_merge_data(path_dwt, df_reference, df_alignment, entity_dwt_apply):\n",
    "    # Synchronize\n",
    "    data_sync = []\n",
    "    for i in range(0, len(path_dwt)):\n",
    "        data_sync.append([path_dwt[i][0],  # The index column is for dropping out duplicates.\n",
    "                         df_reference[entity_dwt_apply].iloc[path_dwt[i][0]],\n",
    "                         df_alignment[entity_dwt_apply].iloc[path_dwt[i][1]]])\n",
    "    df_sync = pd.DataFrame(data=data_sync,\n",
    "                           columns=['Index', \n",
    "                                    'Reference '+entity_dwt_apply, \n",
    "                                    'Alignment '+entity_dwt_apply]).dropna()\n",
    "    df_sync = df_sync.drop_duplicates(subset=['Index']) # Drop the duplicates according to the index of the reference.\n",
    "    df_sync = df_sync.reset_index(drop=True)\n",
    "    # Merge/Align\n",
    "    df_sync['Avg'+entity_dwt_apply] = df_sync.loc[:, ['Reference '+entity_dwt_apply, 'Alignment '+entity_dwt_apply]].mean(axis = 1)\n",
    "    # Calculate the difference\n",
    "    df_sync['Diff'+entity_dwt_apply] = df_sync['Reference '+entity_dwt_apply] - df_sync['Alignment '+entity_dwt_apply]\n",
    "    \n",
    "    return df_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b321f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronize the given 2 eyes' data.\n",
    "def dtw_sync(left_eye_file_path, right_eye_file_path):\n",
    "    # Prepare for the sychronization.\n",
    "    path, df_reference, df_alignment, entity_dwt_apply_dia, entity_dwt_apply_conf, df_origin, SR_SYNC = _pre_dtw_sync(left_eye_file_path = left_eye_file_path, \n",
    "                                                                                                                  right_eye_file_path = right_eye_file_path)\n",
    "\n",
    "    \n",
    "    # Synchronize, merge, and label data.\n",
    "    df_sync_dia = _dtw_synchronize_merge_data(path_dwt=path,\n",
    "                                              df_reference=df_reference,\n",
    "                                              df_alignment=df_alignment,\n",
    "                                              entity_dwt_apply=entity_dwt_apply_dia)\n",
    "    df_sync_conf = _dtw_synchronize_merge_data(path_dwt=path,\n",
    "                                               df_reference=df_reference,\n",
    "                                               df_alignment=df_alignment,\n",
    "                                               entity_dwt_apply=entity_dwt_apply_conf)\n",
    "    \n",
    "    # Integrate into one dataframe.\n",
    "    df_sync = pd.DataFrame()\n",
    "    df_sync['Timestamp'] = df_origin['Timestamp']\n",
    "    df_sync['Confidence'] = df_sync_conf['AvgConfidence']\n",
    "    df_sync['Diameter'] = df_sync_dia['AvgDiameter']\n",
    "    df_sync['Event'] = df_origin['Event']\n",
    "    df_sync['DiffDiameter'] = df_sync_dia['DiffDiameter']\n",
    "    \n",
    "    # Output and save into a csv file.\n",
    "    df_export = df_sync.copy()\n",
    "    file_name = left_eye_file_path.split('/')[-2:]\n",
    "\n",
    "    folder_path = '../Data/PreprocessedData/' + file_name[0] + '/'\n",
    "    if os.path.exists(folder_path) is False:\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    write_file_name = 'synchronized_' + str(SR_SYNC) + 'Hz.csv'\n",
    "    write_file_path = folder_path + write_file_name\n",
    "    df_export.to_csv(write_file_path)\n",
    "    \n",
    "    return df_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896c170",
   "metadata": {},
   "source": [
    "### Merge Two Eyes' Data using Timestamps, Interpolate, and Calculate the Difference\n",
    "Reference: a blog introducing 2 sensors' data fusion - https://stackoverflow.com/questions/14079766/synchronize-dataset-multiple-users-multiple-timestamps\n",
    "\n",
    "#### Configuration Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b7c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TS = 'Timestamp'\n",
    "CF = 'Confidence'\n",
    "DM = 'Diameter'\n",
    "LDM = 'Left Diameter'\n",
    "RDM = 'Right Diameter'\n",
    "LCF = 'Left Confidence'\n",
    "RCF = 'Right Confidence'\n",
    "AVEDM = 'Average Diameter'\n",
    "AVECF = 'Average Confidence'\n",
    "DIFFDM = 'Difference Diameter'  # By my default: Left - Right - unit in pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a4a93",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed6af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_timestamps_sync(left_eye_file_path, right_eye_file_path):\n",
    "    # Read data from csv into dataframes.\n",
    "    df_left = pd.read_csv(left_eye_file_path)\n",
    "    df_right = pd.read_csv(right_eye_file_path)\n",
    "    df_left = df_left[[TS,CF,DM]].copy()\n",
    "    df_right = df_right[[TS,CF,DM]].copy()\n",
    "    \n",
    "    # Get the diameter data indexed by timestamps.\n",
    "    left_diameters = df_left[DM].to_numpy() \n",
    "    series_left = pd.Series(left_diameters, index=df_left[TS])\n",
    "    right_diameters = df_right[DM].to_numpy() \n",
    "    series_right = pd.Series(right_diameters, index=df_right[TS])\n",
    "    \n",
    "    # Synchronize 2 eyes' data by listing all timestamps, this process is actually a up-sampling.\n",
    "    df_sync = pd.DataFrame([series_left, series_right]).T.sort_index()\n",
    "    df_sync = df_sync.rename(columns={0: LDM, 1: RDM})\n",
    "\n",
    "    # Interpolate all the NAN values using 'Spline 3' method with a bi-directional strategy to fill both the first and the last NAN values.\n",
    "    df_sync = df_sync.interpolate(method='spline', order=3, limit_direction='both', axis=0)\n",
    "    \n",
    "    # Align the confidence values according to the timestamps. \n",
    "    # Reference: Adding values in new column based on indexes with pandas in python. https://stackoverflow.com/questions/45636105/adding-values-in-new-column-based-on-indexes-with-pandas-in-python\n",
    "    df_sync[LCF] = df_sync.index.to_series().map(df_left.set_index(TS)[CF])\n",
    "    df_sync[RCF] = df_sync.index.to_series().map(df_right.set_index(TS)[CF])\n",
    "    \n",
    "    # Interpolate NAN values using the normal linear method.\n",
    "    df_sync[LCF] = df_sync[LCF].interpolate(method='linear', limit_direction='both', axis=0)\n",
    "    df_sync[RCF] = df_sync[RCF].interpolate(method='linear', limit_direction='both', axis=0)\n",
    "    \n",
    "#     # Get the difference and average of two eyes' diameter data and confidence values.\n",
    "#     df_sync[AVEDM] = (df_sync[LDM] + df_sync[RDM]) / 2\n",
    "#     df_sync[DIFFDM] = df_sync[LDM] - df_sync[RDM]\n",
    "#     df_sync[AVECF] = (df_sync[LCF] + df_sync[RCF]) / 2\n",
    "    \n",
    "    return df_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0328f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug area\n",
    "# left = \"../Data/RawData4ML/VersionOctober/05-10-13-15-lowlux-nothing/left2D_89Hz.csv\"\n",
    "# right = \"../Data/RawData4ML/VersionOctober/05-10-13-15-lowlux-nothing/right2D_75Hz.csv\"\n",
    "# df = upsample_timestamps_sync(left, right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df6e65",
   "metadata": {},
   "source": [
    "### Deblinks and Blinking Rate Extraction\n",
    "\n",
    "Reference: Check how David Linderbaure's group deal with the blinks. Use confidence to identify blinks. They removed the data within 200ms. However, they did not interpolate the eliminated ones. Here I clean data before and after 200ms of blinks. The input is the numpy data list of the \"confidence\" conlumn. Then return a list that marks which indecies are blinks. \n",
    "\n",
    "\n",
    "#### Configuration Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0fdfce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deblink.\n",
    "BLINK_EXTENSION_TIMEWINDOW = 0.2 # 200ms\n",
    "MIN_CF = 0.25\n",
    "MIN_NUM_SAMPLE_BLINKS = 2\n",
    "\n",
    "# Smooth.\n",
    "WIN_TYPE = 'hann'\n",
    "HANN_WINDOW_SIZE = 5\n",
    "\n",
    "# Interpolate.\n",
    "CURVE_TYPE = 'spline'\n",
    "CURVE_ORDER = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734bcb1",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12eec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deblinks(df_input, confidence_column_label, diameter_column_label):\n",
    "    # Feed in the dataframe to be processed and specific columns of the same eye/the averaged eye.\n",
    "    # To be noted that the input was indexed by the timestamps. \n",
    "    # One Has to use the form of df_input[Col][df_input.index[i]] to reach the ith element.\n",
    "    \n",
    "    # Parameter initilization\n",
    "    blinks = []\n",
    "    num_samples = len(df_input)\n",
    "    i = 0 # The index starter.\n",
    "    \n",
    "    # Identify the blinks according to the low confidence values.\n",
    "    while i < num_samples:\n",
    "        if df_input[confidence_column_label][df_input.index[i]] < MIN_CF and i < num_samples -1:\n",
    "            offset = 1\n",
    "            next_data = df_input[confidence_column_label][df_input.index[i+offset]]\n",
    "            while next_data < MIN_CF:\n",
    "                offset = offset + 1\n",
    "                if i + offset >= num_samples:\n",
    "                    break\n",
    "                next_data = df_input[confidence_column_label][df_input.index[i+offset]]\n",
    "            \n",
    "            # Judge whether the current index exceeds the 200ms time window.\n",
    "            if offset >= MIN_NUM_SAMPLE_BLINKS:\n",
    "                blinks.append((i, offset))\n",
    "            \n",
    "            i = i + offset\n",
    "        else:\n",
    "            i = i + 1\n",
    "    \n",
    "    # Mark data before and after BLINK_EXTENSION_TIMEWINDOW of samples.\n",
    "    for j in range(len(blinks)):\n",
    "        blink_index = blinks[j][0]\n",
    "        blink_length = blinks[j][1]\n",
    "        \n",
    "        # Mark blinks within the searched area as np.nan values.\n",
    "        for j in range(0, blink_length):\n",
    "            df_input[diameter_column_label][df_input.index[blink_index + j]] = np.nan\n",
    "        \n",
    "        # Search for the time window with a length of 200ms. Then also mark blinks with np.nan values for the convenience of interplating.\n",
    "        # Decremnenting.\n",
    "        blink_start_timestamp = df_input.index[blink_index]\n",
    "        k_dec = 0\n",
    "        decrement_index = blink_index - k_dec\n",
    "        # Controlled by the boundary conditions.\n",
    "        while decrement_index >= 0:\n",
    "            dec_timestamp = df_input.index[decrement_index]\n",
    "            if blink_start_timestamp - dec_timestamp >= BLINK_EXTENSION_TIMEWINDOW:\n",
    "                break\n",
    "            else:\n",
    "                df_input[diameter_column_label][df_input.index[decrement_index]] = np.nan\n",
    "                k_dec = k_dec + 1\n",
    "                decrement_index = blink_index - k_dec\n",
    "        \n",
    "        # Incrementing\n",
    "        blink_stop_timestamp = df_input.index[blink_index + blink_length]\n",
    "        k_inc = 0\n",
    "        increment_index = blink_index + blink_length + k_inc\n",
    "        while increment_index < num_samples:\n",
    "            inc_timestamp = df_input.index[increment_index]\n",
    "            if inc_timestamp - blink_stop_timestamp >= BLINK_EXTENSION_TIMEWINDOW:\n",
    "                break\n",
    "            else:\n",
    "                df_input[diameter_column_label][df_input.index[increment_index]] = np.nan\n",
    "                k_inc = k_inc + 1\n",
    "                increment_index = blink_index + blink_length + k_inc\n",
    "    \n",
    "    \n",
    "    # Smooth the data - [Suspended] - Since the objective of freqeuncy-based analysis was to detect singularities, smooth should not be included here.\n",
    "    # Besides, in Marshall's patent, https://patentimages.storage.googleapis.com/91/2f/5f/236d6711dcf6b6/US6090051.pdf, smooth was not applied.\n",
    "#     df_input[diameter_column_label] = df_input[diameter_column_label].rolling(window=HANN_WINDOW_SIZE, center=True, win_type=WIN_TYPE).mean()\n",
    "    \n",
    "    # Interpolate the data - included in Marshall's patent.\n",
    "    df_input[diameter_column_label] = df_input[diameter_column_label].interpolate(method=CURVE_TYPE,order=CURVE_ORDER, limit_direction='both', axis=0)\n",
    "    \n",
    "    df_output = df_input.copy()\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3de654ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug area.\n",
    "# # Example debug, to be deleted later.\n",
    "# df_ = deblinks(df, LCF, LDM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87d550",
   "metadata": {},
   "source": [
    "### [Suspended] Artefact Rejection\n",
    "This part is suspended for better results of the singularity detection.\n",
    "#### Configuration Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9276a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAMPLE_WIN_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a9b76",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "900859ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This part is directly cited from Sam's work.\n",
    "\n",
    "# Filtering outliers \n",
    "# Lan et al. 2020 - median filter with sliding window of 10s\n",
    "# Testing with numba optimised for-loop implementation of a Hampel Filter\n",
    "# Note to self: I think this filter is also commonly used for pupil diameter filtering\n",
    "@jit(nopython=True)\n",
    "def hampel_filter_forloop_numba(input_series, window_size, n_sigmas=3):\n",
    "    \n",
    "    n = len(input_series)\n",
    "    new_series = input_series.copy()\n",
    "    k = 1.4826 # scale factor for Gaussian distribution\n",
    "    indices = []\n",
    "    \n",
    "    for i in range((window_size),(n - window_size)):\n",
    "        x0 = np.nanmedian(input_series[(i - window_size):(i + window_size)])\n",
    "        S0 = k * np.nanmedian(np.abs(input_series[(i - window_size):(i + window_size)] - x0))\n",
    "        if (np.abs(input_series[i] - x0) > n_sigmas * S0):\n",
    "            new_series[i] = x0\n",
    "            indices.append(i)\n",
    "    \n",
    "    return new_series, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ad3e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rej_artifact(df_input, diameter_column_label):\n",
    "    df_ouput = df_input.copy()\n",
    "    x_, outlier_x_ = hampel_filter_forloop_numba(df_ouput[diameter_column_label].to_numpy(), HAMPLE_WIN_SIZE)\n",
    "    df_ouput[diameter_column_label] = x_.tolist()\n",
    "    return df_ouput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155736b7",
   "metadata": {},
   "source": [
    "## Segmenting smaller windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abbd51",
   "metadata": {},
   "source": [
    "## Wavelet Coefficient Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00a372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7ecf5ec",
   "metadata": {},
   "source": [
    "## Run in Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d561063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argument configuration.\n",
    "CONF_2DMODE = '2D'\n",
    "CONF_LEFT = 'left'\n",
    "CONF_RIGHT = 'right'\n",
    "raw_data_path = '../Data/RawData4ML/VersionOctober/' # In this case, the dirpath is mypath, dirnames contains sub-folders's names I need, and no filenames since there is no files there.\n",
    "dir_features = '../Data/Results/'\n",
    "\n",
    "# Create a dataframe to store results details. TODO: to be modified to align to short time windows. Columns - features; Rows - short segmentations.\n",
    "df_features = pd.DataFrame(columns=['Filename', 'Max level value', \n",
    "                                   'IPA 2', 'IPA 3', 'IPA 4', 'IPA 5', 'IPA 6', 'IPA 7', 'IPA 8', 'IPA 9',\n",
    "                                   'LHIPA 2', 'LHIPA 3', 'LHIPA 4', 'LHIPA 5', 'LHIPA 6', 'LHIPA 7', 'LHIPA 8', 'LHIPA 9'])  # Needs to be changed according to requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d7705e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all directory names\n",
    "dirs_list = []\n",
    "for (dir_path, dir_names, file_names) in walk(raw_data_path):\n",
    "    dirs_list.extend(dir_names)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7524cc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 672 ms\n",
      "Wall time: 661 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Traverse all the file names in a given directory.\n",
    "for dir_name in dirs_list:\n",
    "    dir_path = raw_data_path + dir_name + '/'\n",
    "    file_names_list = []\n",
    "    for (_, _, file_names) in walk(dir_path):\n",
    "        file_names_list.extend(file_names)\n",
    "    \n",
    "    # Find the targetted files.\n",
    "    for file_name in file_names:\n",
    "        if CONF_2DMODE in file_name:\n",
    "            if CONF_LEFT in file_name:\n",
    "                file_path_left = dir_path + file_name\n",
    "            elif CONF_RIGHT in file_name:\n",
    "                file_path_right = dir_path + file_name\n",
    "      \n",
    "    # Synchronize 2 eyes' data.\n",
    "    df_sync = upsample_timestamps_sync(left_eye_file_path=file_path_left, right_eye_file_path=file_path_right)\n",
    "    \n",
    "    # Deblink, and interpolate.\n",
    "    # Process the left eye data.\n",
    "    df_deblink = deblinks(df_input=df_sync, confidence_column_label=LCF, diameter_column_label=LDM)\n",
    "    # Process the right eye data.\n",
    "    df_deblink = deblinks(df_input=df_deblink, confidence_column_label=RCF, diameter_column_label=RDM)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6eb18b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left Diameter</th>\n",
       "      <th>Right Diameter</th>\n",
       "      <th>Left Confidence</th>\n",
       "      <th>Right Confidence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>352723.490801</th>\n",
       "      <td>43.808819</td>\n",
       "      <td>45.411758</td>\n",
       "      <td>0.562328</td>\n",
       "      <td>0.602150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352723.492894</th>\n",
       "      <td>43.873134</td>\n",
       "      <td>41.347263</td>\n",
       "      <td>0.575907</td>\n",
       "      <td>0.602150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352723.498762</th>\n",
       "      <td>44.005180</td>\n",
       "      <td>45.536383</td>\n",
       "      <td>0.589486</td>\n",
       "      <td>0.708128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352723.509503</th>\n",
       "      <td>43.845901</td>\n",
       "      <td>45.697473</td>\n",
       "      <td>0.551540</td>\n",
       "      <td>0.814106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352723.517701</th>\n",
       "      <td>44.138456</td>\n",
       "      <td>45.596012</td>\n",
       "      <td>0.547389</td>\n",
       "      <td>0.920084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352783.046935</th>\n",
       "      <td>44.030834</td>\n",
       "      <td>50.373090</td>\n",
       "      <td>0.671217</td>\n",
       "      <td>0.618062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352783.049092</th>\n",
       "      <td>44.351574</td>\n",
       "      <td>50.720688</td>\n",
       "      <td>0.592153</td>\n",
       "      <td>0.643356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352783.060591</th>\n",
       "      <td>43.223335</td>\n",
       "      <td>50.387328</td>\n",
       "      <td>0.513089</td>\n",
       "      <td>0.604240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352783.064396</th>\n",
       "      <td>44.410248</td>\n",
       "      <td>49.592499</td>\n",
       "      <td>0.467182</td>\n",
       "      <td>0.565125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352783.070567</th>\n",
       "      <td>44.258816</td>\n",
       "      <td>50.397907</td>\n",
       "      <td>0.421274</td>\n",
       "      <td>0.565125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9801 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Left Diameter  Right Diameter  Left Confidence  \\\n",
       "Timestamp                                                       \n",
       "352723.490801      43.808819       45.411758         0.562328   \n",
       "352723.492894      43.873134       41.347263         0.575907   \n",
       "352723.498762      44.005180       45.536383         0.589486   \n",
       "352723.509503      43.845901       45.697473         0.551540   \n",
       "352723.517701      44.138456       45.596012         0.547389   \n",
       "...                      ...             ...              ...   \n",
       "352783.046935      44.030834       50.373090         0.671217   \n",
       "352783.049092      44.351574       50.720688         0.592153   \n",
       "352783.060591      43.223335       50.387328         0.513089   \n",
       "352783.064396      44.410248       49.592499         0.467182   \n",
       "352783.070567      44.258816       50.397907         0.421274   \n",
       "\n",
       "               Right Confidence  \n",
       "Timestamp                        \n",
       "352723.490801          0.602150  \n",
       "352723.492894          0.602150  \n",
       "352723.498762          0.708128  \n",
       "352723.509503          0.814106  \n",
       "352723.517701          0.920084  \n",
       "...                         ...  \n",
       "352783.046935          0.618062  \n",
       "352783.049092          0.643356  \n",
       "352783.060591          0.604240  \n",
       "352783.064396          0.565125  \n",
       "352783.070567          0.565125  \n",
       "\n",
       "[9801 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_deblink"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

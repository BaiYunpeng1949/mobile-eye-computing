{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537937a8",
   "metadata": {},
   "source": [
    "# The Pupil Features Extracted in Batch Pipeline.\n",
    "## Introuduction\n",
    "The features to be extracted includes blinking rate and chunks of wavelet coefficients.\n",
    "\n",
    "## Reference\n",
    "1. The previous codes.https://github.com/BaiYunpeng1949/MobileEyeComputing/tree/master/ProcessDataNTestAlgorithm\n",
    "2. TODO: add papers here.\n",
    "3. My work: https://docs.google.com/document/d/1oLv3oJQLjst1_pYgd_UA3RRL1fRGSbZ6uvlMuxmZR2k/edit#heading=h.r01ccf7ox05g\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75874ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "from os import walk\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62025f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(ax, data1, data2, param_dict):\n",
    "    out = ax.plot(data1, data2, **param_dict)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c74efe",
   "metadata": {},
   "source": [
    "## File-wise/Task-wise Data Pre-processing\n",
    "### [Suspended] Left and Right Eye Synchronization and Calculate the Difference\n",
    "Since I now implement ML/AI models to do a more inclusive analysis where more features could be analyzed, I will use the feature of the difference between left and right eye data.\n",
    "\n",
    "\n",
    "1. Referenced from the previous work located as: https://github.com/BaiYunpeng1949/MobileEyeComputing/blob/master/ProcessDataNTestAlgorithm/LeftRightEyesSyncData.ipynb\n",
    "\n",
    "2. The difference between two eyes were found correlated with cognitive workload estimation as well, see the related paper: Optimizing the usage of pupillary based indicators for cognitive workload. Reading link: https://docs.google.com/document/d/1jBezc9kqaziGlWk6sSgCvyjHTlpD5G6OoNZyh7K2HIo/edit#heading=h.qqfv1ot6zjd8\n",
    "\n",
    "Besides, there is a considerable difference in pupil size variation for right and left eyes of the participants. See the paper: Exploring pupil size variation as a cognitive load indicator in visualization studies, link: https://drive.google.com/file/d/1z8O1NGNYA87La-CVMSr7cQkJ-VVOOUSe/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "312ae7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pre_dtw_sync(left_eye_file_path, right_eye_file_path):   \n",
    "    # Configure the parameters\n",
    "    entity_dwt_align = 'Timestamp'\n",
    "    entity_dwt_apply_dia = 'Diameter'\n",
    "    entity_dwt_apply_conf = 'Confidence'\n",
    "    left_eye = 'left'\n",
    "    right_eye = 'right'\n",
    "    \n",
    "    # Tag labels\n",
    "    SAMPLING_RATE_LEFT = int((left_eye_file_path.split('_')[-1]).split('Hz')[0])\n",
    "    data_left = pd.read_csv(left_eye_file_path)\n",
    "\n",
    "    SAMPLING_RATE_RIGHT = int((right_eye_file_path.split('_')[-1]).split('Hz')[0])\n",
    "    data_right = pd.read_csv(right_eye_file_path)\n",
    "    \n",
    "    df_left = data_left[['Timestamp','Confidence','Diameter','Event']].copy()\n",
    "    df_right = data_right[['Timestamp','Confidence','Diameter','Event']].copy()\n",
    "    \n",
    "    # Determine the left and right eye data's size first: which one is bigger?\n",
    "    # Identify the number of elements in the left/right eyes data. If one applies the up-sampling method, the larger eye needs to be put in the first argument.\n",
    "    len_left = len(df_left)\n",
    "    len_right = len(df_right)\n",
    "    if len_left >= len_right:\n",
    "        df_reference = df_left.copy()  # df_reference: the one that being put in the first argument, as a reference.\n",
    "        df_alignment = df_right.copy() # df_alignment: the one that being put in the second argument, to be aligned to the reference.\n",
    "        df_origin = df_left.copy()\n",
    "        SR_SYNC = SAMPLING_RATE_LEFT\n",
    "    elif len_left < len_right:\n",
    "        df_reference = df_right.copy()\n",
    "        df_alignment = df_left.copy()\n",
    "        df_origin = df_right.copy()\n",
    "        SR_SYNC = SAMPLING_RATE_RIGHT\n",
    "    \n",
    "    # Calculate for warping.\n",
    "    distance, path = fastdtw(df_reference[entity_dwt_align], df_alignment[entity_dwt_align], dist=euclidean)\n",
    "    \n",
    "    return path, df_reference, df_alignment, entity_dwt_apply_dia, entity_dwt_apply_conf, df_origin, SR_SYNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab963e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function scnchronize and align/merge 2 eyes' numerical values, including diamter and confidence values.\n",
    "# The method of getting the average/mean value of 2 eyes' data as computing target is referenced from the mention in LHIPA.\n",
    "def _dtw_synchronize_merge_data(path_dwt, df_reference, df_alignment, entity_dwt_apply):\n",
    "    # Synchronize\n",
    "    data_sync = []\n",
    "    for i in range(0, len(path_dwt)):\n",
    "        data_sync.append([path_dwt[i][0],  # The index column is for dropping out duplicates.\n",
    "                         df_reference[entity_dwt_apply].iloc[path_dwt[i][0]],\n",
    "                         df_alignment[entity_dwt_apply].iloc[path_dwt[i][1]]])\n",
    "    df_sync = pd.DataFrame(data=data_sync,\n",
    "                           columns=['Index', \n",
    "                                    'Reference '+entity_dwt_apply, \n",
    "                                    'Alignment '+entity_dwt_apply]).dropna()\n",
    "    df_sync = df_sync.drop_duplicates(subset=['Index']) # Drop the duplicates according to the index of the reference.\n",
    "    df_sync = df_sync.reset_index(drop=True)\n",
    "    # Merge/Align\n",
    "    df_sync['Avg'+entity_dwt_apply] = df_sync.loc[:, ['Reference '+entity_dwt_apply, 'Alignment '+entity_dwt_apply]].mean(axis = 1)\n",
    "    # Calculate the difference\n",
    "    df_sync['Diff'+entity_dwt_apply] = df_sync['Reference '+entity_dwt_apply] - df_sync['Alignment '+entity_dwt_apply]\n",
    "    \n",
    "    return df_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b321f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synchronize the given 2 eyes' data.\n",
    "def dtw_sync(left_eye_file_path, right_eye_file_path):\n",
    "    # Prepare for the sychronization.\n",
    "    path, df_reference, df_alignment, entity_dwt_apply_dia, entity_dwt_apply_conf, df_origin, SR_SYNC = _pre_dtw_sync(left_eye_file_path = left_eye_file_path, \n",
    "                                                                                                                  right_eye_file_path = right_eye_file_path)\n",
    "\n",
    "    \n",
    "    # Synchronize, merge, and label data.\n",
    "    df_sync_dia = _dtw_synchronize_merge_data(path_dwt=path,\n",
    "                                              df_reference=df_reference,\n",
    "                                              df_alignment=df_alignment,\n",
    "                                              entity_dwt_apply=entity_dwt_apply_dia)\n",
    "    df_sync_conf = _dtw_synchronize_merge_data(path_dwt=path,\n",
    "                                               df_reference=df_reference,\n",
    "                                               df_alignment=df_alignment,\n",
    "                                               entity_dwt_apply=entity_dwt_apply_conf)\n",
    "    \n",
    "    # Integrate into one dataframe.\n",
    "    df_sync = pd.DataFrame()\n",
    "    df_sync['Timestamp'] = df_origin['Timestamp']\n",
    "    df_sync['Confidence'] = df_sync_conf['AvgConfidence']\n",
    "    df_sync['Diameter'] = df_sync_dia['AvgDiameter']\n",
    "    df_sync['Event'] = df_origin['Event']\n",
    "    df_sync['DiffDiameter'] = df_sync_dia['DiffDiameter']\n",
    "    \n",
    "    # Output and save into a csv file.\n",
    "    df_export = df_sync.copy()\n",
    "    file_name = left_eye_file_path.split('/')[-2:]\n",
    "\n",
    "    folder_path = '../Data/PreprocessedData/' + file_name[0] + '/'\n",
    "    if os.path.exists(folder_path) is False:\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    write_file_name = 'synchronized_' + str(SR_SYNC) + 'Hz.csv'\n",
    "    write_file_path = folder_path + write_file_name\n",
    "    df_export.to_csv(write_file_path)\n",
    "    \n",
    "    return df_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21abdf8",
   "metadata": {},
   "source": [
    "### Merge Two Eyes' Data using Timestamps, Interpolate, and Calculate the Difference\n",
    "Reference: a blog introducing 2 sensors' data fusion - https://stackoverflow.com/questions/14079766/synchronize-dataset-multiple-users-multiple-timestamps\n",
    "\n",
    "#### Configuration Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "efb0612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TS = 'Timestamp'\n",
    "CF = 'Confidence'\n",
    "DM = 'Diameter'\n",
    "LDM = 'Left Diameter'\n",
    "RDM = 'Right Diameter'\n",
    "LCF = 'Left Confidence'\n",
    "RCF = 'Right Confidence'\n",
    "AVEDM = 'Average Diameter'\n",
    "AVECF = 'Average Confidence'\n",
    "DIFFDM = 'Difference Diameter'  # By my default: Left - Right - unit in pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef805de6",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "73550307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_timestamps_sync(left_eye_file_path, right_eye_file_path):\n",
    "    # Read data from csv into dataframes.\n",
    "    df_left = pd.read_csv(left_eye_file_path)\n",
    "    df_right = pd.read_csv(right_eye_file_path)\n",
    "    df_left = df_left[[TS,CF,DM]].copy()\n",
    "    df_right = df_right[[TS,CF,DM]].copy()\n",
    "    \n",
    "    # Get the diameter data indexed by timestamps.\n",
    "    left_diameters = df_left[DM].to_numpy() \n",
    "    series_left = pd.Series(left_diameters, index=df_left[TS])\n",
    "    right_diameters = df_right[DM].to_numpy() \n",
    "    series_right = pd.Series(right_diameters, index=df_right[TS])\n",
    "    \n",
    "    # Synchronize 2 eyes' data by listing all timestamps, this process is actually a up-sampling.\n",
    "    df_sync = pd.DataFrame([series_left, series_right]).T.sort_index()\n",
    "    df_sync = df_sync.rename(columns={0: LDM, 1: RDM})\n",
    "\n",
    "    # Interpolate all the NAN values using 'Spline 3' method with a bi-directional strategy to fill both the first and the last NAN values.\n",
    "    df_sync = df_sync.interpolate(method='spline', order=3, limit_direction='both', axis=0)\n",
    "    \n",
    "    # Align the confidence values according to the timestamps. \n",
    "    # Reference: Adding values in new column based on indexes with pandas in python. https://stackoverflow.com/questions/45636105/adding-values-in-new-column-based-on-indexes-with-pandas-in-python\n",
    "    df_sync[LCF] = df_sync.index.to_series().map(df_left.set_index(TS)[CF])\n",
    "    df_sync[RCF] = df_sync.index.to_series().map(df_right.set_index(TS)[CF])\n",
    "    \n",
    "    # Interpolate NAN values using the normal linear method.\n",
    "    df_sync[LCF] = df_sync[LCF].interpolate(method='linear', limit_direction='both', axis=0)\n",
    "    df_sync[RCF] = df_sync[RCF].interpolate(method='linear', limit_direction='both', axis=0)\n",
    "    \n",
    "    # Get the difference and average of two eyes' diameter data and confidence values.\n",
    "    df_sync[AVEDM] = (df_sync[LDM] + df_sync[RDM]) / 2\n",
    "    df_sync[DIFFDM] = df_sync[LDM] - df_sync[RDM]\n",
    "    df_sync[AVECF] = (df_sync[LCF] + df_sync[RCF]) / 2\n",
    "    \n",
    "    return df_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df6e65",
   "metadata": {},
   "source": [
    "### Deblinks and Blinking Rate Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84ba7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bcdd438",
   "metadata": {},
   "source": [
    "### Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab57a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "566c00a3",
   "metadata": {},
   "source": [
    "### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4b396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d87d550",
   "metadata": {},
   "source": [
    "### Artefact Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900859ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2abbd51",
   "metadata": {},
   "source": [
    "## Wavelet Coefficient Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00a372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7ecf5ec",
   "metadata": {},
   "source": [
    "## Run in Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d561063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argument configuration.\n",
    "CONF_2DMODE = '2D'\n",
    "CONF_LEFT = 'left'\n",
    "CONF_RIGHT = 'right'\n",
    "raw_data_path = '../Data/RawData4ML/VersionOctober/' # In this case, the dirpath is mypath, dirnames contains sub-folders's names I need, and no filenames since there is no files there.\n",
    "dir_features = '../Data/Results/'\n",
    "\n",
    "# Create a dataframe to store results details. TODO: to be modified to align to short time windows. Columns - features; Rows - short segmentations.\n",
    "df_features = pd.DataFrame(columns=['Filename', 'Max level value', \n",
    "                                   'IPA 2', 'IPA 3', 'IPA 4', 'IPA 5', 'IPA 6', 'IPA 7', 'IPA 8', 'IPA 9',\n",
    "                                   'LHIPA 2', 'LHIPA 3', 'LHIPA 4', 'LHIPA 5', 'LHIPA 6', 'LHIPA 7', 'LHIPA 8', 'LHIPA 9'])  # Needs to be changed according to requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d7705e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all directory names\n",
    "dirs_list = []\n",
    "for (dir_path, dir_names, file_names) in walk(raw_data_path):\n",
    "    dirs_list.extend(dir_names)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7524cc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 78.1 ms\n",
      "Wall time: 78.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Traverse all the file names in a given directory.\n",
    "for dir_name in dirs_list:\n",
    "    dir_path = raw_data_path + dir_name + '/'\n",
    "    file_names_list = []\n",
    "    for (_, _, file_names) in walk(dir_path):\n",
    "        file_names_list.extend(file_names)\n",
    "    \n",
    "    # Find the targetted files.\n",
    "    for file_name in file_names:\n",
    "        if CONF_2DMODE in file_name:\n",
    "            if CONF_LEFT in file_name:\n",
    "                file_path_left = dir_path + file_name\n",
    "            elif CONF_RIGHT in file_name:\n",
    "                file_path_right = dir_path + file_name\n",
    "      \n",
    "    # We suspended the two eyes' synchronization for extracting more information because the difference of two pupil was also recognized as an indicator of cognitive workload.\n",
    "    # TODO: reference is needed here.\n",
    "    # Synchronize 2 eyes' data.\n",
    "    df_sync = upsample_timestamps_sync(left_eye_file_path=file_path_left, right_eye_file_path=file_path_right)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "48dda6f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left Diameter</th>\n",
       "      <th>Right Diameter</th>\n",
       "      <th>Left Confidence</th>\n",
       "      <th>Right Confidence</th>\n",
       "      <th>Average Diameter</th>\n",
       "      <th>Difference Diameter</th>\n",
       "      <th>Average Confidence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>352723.490801</th>\n",
       "      <td>43.808819</td>\n",
       "      <td>45.411758</td>\n",
       "      <td>0.562328</td>\n",
       "      <td>0.602150</td>\n",
       "      <td>44.610288</td>\n",
       "      <td>-1.602939</td>\n",
       "      <td>0.582239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352723.492894</th>\n",
       "      <td>43.873134</td>\n",
       "      <td>41.347263</td>\n",
       "      <td>0.575907</td>\n",
       "      <td>0.602150</td>\n",
       "      <td>42.610198</td>\n",
       "      <td>2.525870</td>\n",
       "      <td>0.589028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352723.498762</th>\n",
       "      <td>44.005180</td>\n",
       "      <td>45.536383</td>\n",
       "      <td>0.589486</td>\n",
       "      <td>0.708128</td>\n",
       "      <td>44.770782</td>\n",
       "      <td>-1.531203</td>\n",
       "      <td>0.648807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352723.509503</th>\n",
       "      <td>43.845901</td>\n",
       "      <td>45.697473</td>\n",
       "      <td>0.551540</td>\n",
       "      <td>0.814106</td>\n",
       "      <td>44.771687</td>\n",
       "      <td>-1.851571</td>\n",
       "      <td>0.682823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352723.517701</th>\n",
       "      <td>44.138456</td>\n",
       "      <td>45.596012</td>\n",
       "      <td>0.547389</td>\n",
       "      <td>0.920084</td>\n",
       "      <td>44.867234</td>\n",
       "      <td>-1.457556</td>\n",
       "      <td>0.733737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352783.046935</th>\n",
       "      <td>44.030834</td>\n",
       "      <td>50.373090</td>\n",
       "      <td>0.671217</td>\n",
       "      <td>0.618062</td>\n",
       "      <td>47.201962</td>\n",
       "      <td>-6.342255</td>\n",
       "      <td>0.644639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352783.049092</th>\n",
       "      <td>44.351574</td>\n",
       "      <td>50.720688</td>\n",
       "      <td>0.592153</td>\n",
       "      <td>0.643356</td>\n",
       "      <td>47.536131</td>\n",
       "      <td>-6.369114</td>\n",
       "      <td>0.617755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352783.060591</th>\n",
       "      <td>43.223335</td>\n",
       "      <td>50.387328</td>\n",
       "      <td>0.513089</td>\n",
       "      <td>0.604240</td>\n",
       "      <td>46.805332</td>\n",
       "      <td>-7.163993</td>\n",
       "      <td>0.558665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352783.064396</th>\n",
       "      <td>44.410248</td>\n",
       "      <td>49.592499</td>\n",
       "      <td>0.467182</td>\n",
       "      <td>0.565125</td>\n",
       "      <td>47.001373</td>\n",
       "      <td>-5.182251</td>\n",
       "      <td>0.516153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352783.070567</th>\n",
       "      <td>44.258816</td>\n",
       "      <td>50.397907</td>\n",
       "      <td>0.421274</td>\n",
       "      <td>0.565125</td>\n",
       "      <td>47.328361</td>\n",
       "      <td>-6.139091</td>\n",
       "      <td>0.493199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9801 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Left Diameter  Right Diameter  Left Confidence  \\\n",
       "Timestamp                                                       \n",
       "352723.490801      43.808819       45.411758         0.562328   \n",
       "352723.492894      43.873134       41.347263         0.575907   \n",
       "352723.498762      44.005180       45.536383         0.589486   \n",
       "352723.509503      43.845901       45.697473         0.551540   \n",
       "352723.517701      44.138456       45.596012         0.547389   \n",
       "...                      ...             ...              ...   \n",
       "352783.046935      44.030834       50.373090         0.671217   \n",
       "352783.049092      44.351574       50.720688         0.592153   \n",
       "352783.060591      43.223335       50.387328         0.513089   \n",
       "352783.064396      44.410248       49.592499         0.467182   \n",
       "352783.070567      44.258816       50.397907         0.421274   \n",
       "\n",
       "               Right Confidence  Average Diameter  Difference Diameter  \\\n",
       "Timestamp                                                                \n",
       "352723.490801          0.602150         44.610288            -1.602939   \n",
       "352723.492894          0.602150         42.610198             2.525870   \n",
       "352723.498762          0.708128         44.770782            -1.531203   \n",
       "352723.509503          0.814106         44.771687            -1.851571   \n",
       "352723.517701          0.920084         44.867234            -1.457556   \n",
       "...                         ...               ...                  ...   \n",
       "352783.046935          0.618062         47.201962            -6.342255   \n",
       "352783.049092          0.643356         47.536131            -6.369114   \n",
       "352783.060591          0.604240         46.805332            -7.163993   \n",
       "352783.064396          0.565125         47.001373            -5.182251   \n",
       "352783.070567          0.565125         47.328361            -6.139091   \n",
       "\n",
       "               Average Confidence  \n",
       "Timestamp                          \n",
       "352723.490801            0.582239  \n",
       "352723.492894            0.589028  \n",
       "352723.498762            0.648807  \n",
       "352723.509503            0.682823  \n",
       "352723.517701            0.733737  \n",
       "...                           ...  \n",
       "352783.046935            0.644639  \n",
       "352783.049092            0.617755  \n",
       "352783.060591            0.558665  \n",
       "352783.064396            0.516153  \n",
       "352783.070567            0.493199  \n",
       "\n",
       "[9801 rows x 7 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869f2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
